{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed7606e-f61f-4230-9fd6-56be5c302d9e",
   "metadata": {},
   "source": [
    "# What is this tutorial\n",
    "This notebook demonstrates how to run a reinforcement learning workflow using ROSE's reinforcement learner. \n",
    "It will show how to:\n",
    "* Define and register the environment and update tasks\n",
    "* Use a stop criterion to terminate training when a target reward is met\n",
    "* Run a sequential reinforcement learning loop with policy gradient methods\n",
    "\n",
    "## Sequential Reinforcement Learner\n",
    "\n",
    "In this example, we will learn how to use ROSE API to build and submit a `single` Reinforcement Learner that either stops when the performance metric threshold is `met` or the number of iterations the user specified is reached (in this case, 100 iterations). This example uses the REINFORCE policy gradient algorithm for the CartPole environment.\n",
    "\n",
    "\n",
    "                                                  ┌────────────────────────┐\n",
    "                                                  │      ENVIRONMENT       │\n",
    "                                                  │ (Run the policy and    │\n",
    "                                                  │   gather experience)   │\n",
    "                                                  └────────────┬───────────┘\n",
    "                                                               │\n",
    "                                                               ▼\n",
    "                                                  ┌────────────────────────┐\n",
    "                                                  │     POLICY UPDATE      │\n",
    "                                                  │ (Update the policy     │\n",
    "                                                  │  using experiences)    │\n",
    "                                                  └────────────┬───────────┘\n",
    "                                                               │\n",
    "                                                               ▼\n",
    "                                                  ┌────────────────────────┐\n",
    "                                                  │      POLICY TEST       │\n",
    "                                                  │ (Test the performance  │\n",
    "                                                  │   of the new policy)   │\n",
    "                                                  └────────────┬───────────┘\n",
    "                                                               │\n",
    "                                                               ▼\n",
    "                                                  ┌────────────────────────┐\n",
    "                                                  │  IMPROVED POLICY LOOP  │\n",
    "                                                  │(Repeat for N iters     │\n",
    "                                                  │   or performance goal) │\n",
    "                                                  └────────────────────────┘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c26fa2",
   "metadata": {},
   "outputs": [],
   "source": "import asyncio\nimport logging\n\nfrom typing import List, Tuple\nfrom dataclasses import dataclass\n\n# Task imports\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport gymnasium as gym\n\n# ROSE top layer imports\nfrom rose.metrics import GREATER_THAN_THRESHOLD\nfrom rose.rl.reinforcement_learner import SequentialReinforcementLearner\n\n# ROSE Bottom layers imports\nfrom radical.asyncflow import WorkflowEngine\nfrom rhapsody.backends import ConcurrentExecutionBackend\nfrom concurrent.futures import ProcessPoolExecutor\nfrom radical.asyncflow.logging import init_default_logger\n\nlogger = logging.getLogger(__name__)"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26380e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    env_id: str = \"CartPole-v1\"\n",
    "    seed: int = 42\n",
    "    hidden_size: int = 128\n",
    "    gamma: float = 0.99\n",
    "    lr: float = 3e-3\n",
    "    episodes: int = 1000\n",
    "    batch_size: int = 10\n",
    "    reward_solve_threshold: float = 475.0\n",
    "    device: str = \"cpu\"\n",
    "    model_path: str = \"cartpole_policy.pt\"\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eb5f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, obs_dim: int, hidden: int, n_actions: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def action_dist(self, obs: np.ndarray) -> torch.distributions.Categorical:\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        logits = self.forward(obs_t)\n",
    "        return torch.distributions.Categorical(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9442f635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m2025-11-04 13:32:08.173\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[root]\u001b[0m │ Logger configured successfully - Console: INFO, File: disabled (N/A), Structured: disabled, Style: modern\n",
      "\u001b[90m2025-11-04 13:32:08.176\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Starting Reinforcement Learning with ROSE...\n",
      "Starting Sequential RL Learner\n",
      "Starting Iteration-0\n",
      "\u001b[90m2025-11-04 13:32:08.180\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:08.325\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000001 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:08.336\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:09.516\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000002 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:09.528\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:09.657\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000003 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (45.5).\n",
      "\u001b[90m2025-11-04 13:32:09.658\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:09.723\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000004 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:09.734\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:09.745\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000005 is in DONE state\n",
      "Starting Iteration-1\n",
      "\u001b[90m2025-11-04 13:32:09.747\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:09.907\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000006 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (52.0).\n",
      "\u001b[90m2025-11-04 13:32:09.908\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:10.000\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000007 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:10.011\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:10.364\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000008 is in DONE state\n",
      "Starting Iteration-2\n",
      "\u001b[90m2025-11-04 13:32:10.367\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:10.472\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000009 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (53.5).\n",
      "\u001b[90m2025-11-04 13:32:10.474\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:10.642\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000010 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:10.653\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:10.674\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000011 is in DONE state\n",
      "Starting Iteration-3\n",
      "\u001b[90m2025-11-04 13:32:10.678\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:10.839\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000012 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (61.6).\n",
      "\u001b[90m2025-11-04 13:32:10.840\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:10.981\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000013 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:10.992\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:11.019\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000014 is in DONE state\n",
      "Starting Iteration-4\n",
      "\u001b[90m2025-11-04 13:32:11.022\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:11.155\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000015 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (50.6).\n",
      "\u001b[90m2025-11-04 13:32:11.156\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:11.281\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000016 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:11.292\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:11.303\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000017 is in DONE state\n",
      "Starting Iteration-5\n",
      "\u001b[90m2025-11-04 13:32:11.306\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:11.470\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000018 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (82.6).\n",
      "\u001b[90m2025-11-04 13:32:11.472\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:11.601\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000019 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:11.612\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:11.621\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000020 is in DONE state\n",
      "Starting Iteration-6\n",
      "\u001b[90m2025-11-04 13:32:11.626\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:11.815\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000021 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (84.4).\n",
      "\u001b[90m2025-11-04 13:32:11.816\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:11.922\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000022 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:11.933\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:11.948\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000023 is in DONE state\n",
      "Starting Iteration-7\n",
      "\u001b[90m2025-11-04 13:32:11.950\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:12.210\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000024 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (138.8).\n",
      "\u001b[90m2025-11-04 13:32:12.211\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:12.352\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000025 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:12.363\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:12.377\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000026 is in DONE state\n",
      "Starting Iteration-8\n",
      "\u001b[90m2025-11-04 13:32:12.380\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:12.607\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000027 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (128.9).\n",
      "\u001b[90m2025-11-04 13:32:12.608\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:12.815\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000028 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:12.826\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:12.850\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000029 is in DONE state\n",
      "Starting Iteration-9\n",
      "\u001b[90m2025-11-04 13:32:12.853\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:13.179\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000030 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (189.4).\n",
      "\u001b[90m2025-11-04 13:32:13.181\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:13.393\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000031 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:13.405\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:13.424\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000032 is in DONE state\n",
      "Starting Iteration-10\n",
      "\u001b[90m2025-11-04 13:32:13.429\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:13.713\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000033 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (175.0).\n",
      "\u001b[90m2025-11-04 13:32:13.715\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:13.992\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000034 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:14.003\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:14.020\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000035 is in DONE state\n",
      "Starting Iteration-11\n",
      "\u001b[90m2025-11-04 13:32:14.023\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:14.189\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000036 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (110.2).\n",
      "\u001b[90m2025-11-04 13:32:14.191\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:14.377\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000037 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:14.388\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:14.398\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000038 is in DONE state\n",
      "Starting Iteration-12\n",
      "\u001b[90m2025-11-04 13:32:14.400\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:14.592\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000039 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (145.9).\n",
      "\u001b[90m2025-11-04 13:32:14.593\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:14.790\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000040 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:14.801\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:14.820\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000041 is in DONE state\n",
      "Starting Iteration-13\n",
      "\u001b[90m2025-11-04 13:32:14.823\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:14.993\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000042 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (128.5).\n",
      "\u001b[90m2025-11-04 13:32:14.995\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:15.232\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000043 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:15.243\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:15.254\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000044 is in DONE state\n",
      "Starting Iteration-14\n",
      "\u001b[90m2025-11-04 13:32:15.256\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:15.476\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000045 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (134.2).\n",
      "\u001b[90m2025-11-04 13:32:15.477\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:15.760\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000046 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:15.771\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:15.788\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000047 is in DONE state\n",
      "Starting Iteration-15\n",
      "\u001b[90m2025-11-04 13:32:15.790\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:16.002\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000048 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (150.3).\n",
      "\u001b[90m2025-11-04 13:32:16.003\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:16.295\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000049 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:16.307\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:16.328\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000050 is in DONE state\n",
      "Starting Iteration-16\n",
      "\u001b[90m2025-11-04 13:32:16.330\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:16.607\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000051 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (197.9).\n",
      "\u001b[90m2025-11-04 13:32:16.609\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:16.987\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000052 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:16.998\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:17.024\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000053 is in DONE state\n",
      "Starting Iteration-17\n",
      "\u001b[90m2025-11-04 13:32:17.027\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:17.334\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000054 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (173.2).\n",
      "\u001b[90m2025-11-04 13:32:17.335\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:17.641\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000055 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:17.653\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:17.667\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000056 is in DONE state\n",
      "Starting Iteration-18\n",
      "\u001b[90m2025-11-04 13:32:17.670\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:17.958\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000057 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is not met yet (160.1).\n",
      "\u001b[90m2025-11-04 13:32:17.959\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['environment'] for execution\n",
      "\u001b[90m2025-11-04 13:32:18.317\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000058 is in DONE state\n",
      "\u001b[90m2025-11-04 13:32:18.328\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['update'] for execution\n",
      "\u001b[90m2025-11-04 13:32:18.343\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000059 is in DONE state\n",
      "Starting Iteration-19\n",
      "\u001b[90m2025-11-04 13:32:18.346\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['check_reward'] for execution\n",
      "\u001b[90m2025-11-04 13:32:18.997\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000060 is in DONE state\n",
      "stop criterion metric: MODEL_REWARD is met with value of: 498.0 . Breaking the active learning loop\n",
      "\u001b[90m2025-11-04 13:32:18.998\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Reinforcement Learning completed!\n",
      "\u001b[90m2025-11-04 13:32:18.998\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Initiating shutdown\n",
      "\u001b[90m2025-11-04 13:32:18.999\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[execution.backend(concurrent)]\u001b[0m │ Concurrent execution backend shutdown complete\n",
      "\u001b[90m2025-11-04 13:32:19.000\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Shutdown completed for all components.\n"
     ]
    }
   ],
   "source": [
    "async def run(rl, **kwargs):\n",
    "\n",
    "    # ========================================================================\n",
    "    # 0. HELPER FUNCTIONS\n",
    "    # ========================================================================\n",
    "    def discount_reward(rewards: List[float], gamma: float) -> List[float]:\n",
    "        g = 0.0\n",
    "        out = []\n",
    "        for r in reversed(rewards):\n",
    "            g = r + gamma * g\n",
    "            out.append(g)\n",
    "        return list(reversed(out))\n",
    "    def run_episode(env, policy: Network, seed: int = None) -> Tuple[List[np.ndarray], List[int], List[float]]:\n",
    "        obs, _ = env.reset(seed=seed)\n",
    "\n",
    "        obs_list, act_list, rew_list = [], [], []\n",
    "        done = False\n",
    "        while not done:\n",
    "            dist = policy.action_dist(np.expand_dims(obs, axis=0))\n",
    "            action = dist.sample().item()\n",
    "            obs_list.append(obs.copy())\n",
    "            act_list.append(action)\n",
    "\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            rew_list.append(float(reward))\n",
    "            obs = next_obs\n",
    "        return obs_list, act_list, rew_list\n",
    "        \n",
    "    # ========================================================================\n",
    "    # 1. ENVIRONMENT TASK\n",
    "    # ========================================================================\n",
    "    @rl.environment_task(as_executable=False)\n",
    "    async def environment(*args, **kwargs) -> dict:\n",
    "        env = gym.make(cfg.env_id)\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        n_actions = env.action_space.n\n",
    "        policy = Network(obs_dim, cfg.hidden_size, n_actions)\n",
    "        try:\n",
    "            policy.load_state_dict(torch.load(cfg.model_path, map_location=\"cpu\"))\n",
    "        except FileNotFoundError:\n",
    "            torch.save(policy.state_dict(), cfg.model_path)\n",
    "        episode_buffer = []\n",
    "        for i in range(cfg.batch_size):\n",
    "            obs, _ = env.reset(seed=cfg.seed+i)\n",
    "\n",
    "            observations, actions, rewards = run_episode(env, policy, seed=cfg.seed+i)\n",
    "            G = discount_reward(rewards, cfg.gamma)\n",
    "            for o, a, g in zip(observations, actions, G):\n",
    "                episode_buffer.append((o, a, g))\n",
    "        obs_batch, act_batch, ret_batch = zip(*episode_buffer)\n",
    "        return {\"observations\": obs_batch, \"actions\": act_batch, \"returns\": ret_batch}\n",
    "\n",
    "    # ========================================================================\n",
    "    # 2. UPDATE TASK\n",
    "    # ========================================================================\n",
    "    @rl.update_task(as_executable=False)\n",
    "    async def update(*args, **kwargs) -> dict:\n",
    "        data = args[0] if args else kwargs.get(\"data\", {})\n",
    "        env = gym.make(cfg.env_id)\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        n_actions = env.action_space.n\n",
    "        policy = Network(obs_dim, cfg.hidden_size, n_actions)\n",
    "        policy.load_state_dict(torch.load(cfg.model_path, map_location=\"cpu\"))\n",
    "        optimizer = optim.Adam(policy.parameters(), lr=cfg.lr)\n",
    "        obs_batch = torch.tensor(np.array(data[\"observations\"]), dtype=torch.float32, device=cfg.device)\n",
    "        act_batch = torch.tensor(data[\"actions\"], dtype=torch.int64, device=cfg.device)\n",
    "        ret_batch = torch.tensor(data[\"returns\"], dtype=torch.float32, device=cfg.device)\n",
    "        ret_batch = (ret_batch - ret_batch.mean()) / (ret_batch.std() + 1e-8)\n",
    "\n",
    "        dists = policy.action_dist(obs_batch)\n",
    "        logp = dists.log_prob(act_batch)\n",
    "        loss = -(logp * ret_batch).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(policy.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        torch.save(policy.state_dict(), cfg.model_path)\n",
    "        return {\"loss\": loss.item()}\n",
    "\n",
    "    # ========================================================================\n",
    "    # 3. STOP CRITERION TASK\n",
    "    # ========================================================================\n",
    "    @rl.as_stop_criterion(metric_name='MODEL_REWARD', threshold=cfg.reward_solve_threshold, operator=GREATER_THAN_THRESHOLD, as_executable=False)\n",
    "    async def check_reward(*args, **kwargs):\n",
    "        env = gym.make(cfg.env_id)\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        n_actions = env.action_space.n\n",
    "        policy = Network(obs_dim, cfg.hidden_size, n_actions)\n",
    "        policy.load_state_dict(torch.load(cfg.model_path, map_location=\"cpu\"))\n",
    "        policy.eval()\n",
    "        rewards = []\n",
    "        for _ in range(cfg.batch_size):\n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "            it_reward = 0.0\n",
    "    \n",
    "            while not done:\n",
    "                with torch.no_grad():\n",
    "                    dist = policy.action_dist(np.expand_dims(obs, axis=0))\n",
    "                    action = torch.argmax(dist.probs).item() \n",
    "    \n",
    "                obs, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                it_reward += float(reward)\n",
    "            rewards.append(it_reward)\n",
    "        avg_reward = float(np.mean(rewards))\n",
    "        return avg_reward\n",
    "\n",
    "    # Run\n",
    "    logger.info(\"Starting Reinforcement Learning with ROSE...\")\n",
    "    await rl.learn(**kwargs)\n",
    "    logger.info(\"Reinforcement Learning completed!\")\n",
    "\n",
    "try:\n",
    "    engine = await ConcurrentExecutionBackend(ProcessPoolExecutor())\n",
    "    asyncflow = await WorkflowEngine.create(engine)\n",
    "    rl = SequentialReinforcementLearner(asyncflow)\n",
    "\n",
    "    init_default_logger(logging.INFO)\n",
    "    await run(rl, max_iter=int(cfg.episodes/cfg.batch_size))\n",
    "except Exception as e:\n",
    "    print(f'Learner Failed with: {e}')\n",
    "finally:\n",
    "    await rl.shutdown()\n",
    "    logging.getLogger().handlers.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
