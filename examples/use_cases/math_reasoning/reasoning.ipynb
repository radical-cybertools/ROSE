{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207b62f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from rose.metrics import GREATER_THAN_THRESHOLD\n",
    "from rose.rl.reinforcement_learner import SequentialReinforcementLearner\n",
    "\n",
    "from radical.asyncflow import WorkflowEngine\n",
    "from radical.asyncflow import ConcurrentExecutionBackend\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from radical.asyncflow.logging import init_default_logger\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5070984",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name=\"roserun-1\"\n",
    "import os\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"HF_TOKEN\"] = userdata.get('hf_token')\n",
    "    userdata.get('hf')\n",
    "except:\n",
    "    os.environ[\"HF_TOKEN\"] = \"\"\n",
    "    os.environ[\"HF_HOME\"] = \"/work/hdd/bdyk/apark4/huggingface\"\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "import trackio\n",
    "import gc\n",
    "trackio.init(project=\"huggingface\", space_id=\"iznoanygod/trackio\", name=run_name, embed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b9452",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = await ConcurrentExecutionBackend(ProcessPoolExecutor())\n",
    "asyncflow = await WorkflowEngine.create(engine)\n",
    "rl = SequentialReinforcementLearner(asyncflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f067d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "max_prompt_length = 1024\n",
    "lora_rank = 16\n",
    "\n",
    "base_model_id=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "lora_id = \"math_lora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ca563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "You must use LaTeX to format mathematical expressions, and you must use \\\\boxed{...} to indicate the final answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21fc340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(expr: str):\n",
    "    match = re.search(r\"\\\\boxed\\{(.+?)\\}\", expr)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def correctness_reward_func(prompts, completions, ground_truth, **kwargs):\n",
    "    rewards = []\n",
    "    for prompt, completion, ground in zip(prompts, completions, ground_truth):\n",
    "        c = get_answer(completion[0][\"content\"])\n",
    "        g = get_answer(ground)\n",
    "        reward = 2.0 if g == c else 0\n",
    "        rewards.append(reward)\n",
    "    return rewards\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3326d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_prompt_completion(example):\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': example['problem']}\n",
    "            \n",
    "        ],\n",
    "        \"ground_truth\": str(example[\"solution\"]).strip(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792eafa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llama_or_latest_checkpoint(\n",
    "    base_model_id: str,\n",
    "    lora_id: str,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "):\n",
    "    \"\"\"\n",
    "    If `output_dir` contains checkpoints for this run, load the latest one.\n",
    "    Otherwise, load the base model from Hugging Face Hub.\n",
    "    \"\"\"\n",
    "\n",
    "    last_checkpoint = None\n",
    "\n",
    "    if os.path.isdir(lora_id):\n",
    "        last_checkpoint = get_last_checkpoint(lora_id)\n",
    "        \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id, padding_side=\"left\", use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    iteration = 0\n",
    "    if last_checkpoint is not None:\n",
    "        print(f\"Found LoRA checkpoint at: {last_checkpoint}\")\n",
    "        print(f\"Loading base model: {base_model_id}\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_id,\n",
    "            dtype=dtype,\n",
    "            device_map=device_map,\n",
    "        )\n",
    "        m = re.search(r\"checkpoint-(\\d+)\", last_checkpoint)\n",
    "        if m:\n",
    "            iteration = int(m.group(1))\n",
    "        else:\n",
    "            print(f\"Warning: could not parse iteration from {basename}, leaving iteration=0\")\n",
    "        # Attach LoRA adapter weights\n",
    "        print(\"Applying LoRA adapter from checkpoint...\")\n",
    "        model = PeftModel.from_pretrained(base_model, last_checkpoint, is_trainable=True)\n",
    "        loaded_from = last_checkpoint\n",
    "    else:\n",
    "        print(f\"No checkpoint found, loading base model: {base_model_id}\")\n",
    "        lora_config = LoraConfig(\n",
    "            r=lora_rank,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_id,\n",
    "            dtype=dtype,\n",
    "            device_map=device_map,\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        loaded_from = base_model_id\n",
    "\n",
    "    return model, tokenizer, loaded_from, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a8c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_count = torch.cuda.device_count()\n",
    "def memory_stats():\n",
    "    print(\"memory allocated: \", [torch.cuda.memory_allocated(i)/1024**2 for i in range(gpu_count)])\n",
    "    print(\"memory reserved: \", [torch.cuda.memory_reserved(i)/1024**2 for i in range(gpu_count)])\n",
    "    for i in range(gpu_count):\n",
    "        print(torch.cuda.memory_summary(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f0c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run(rl, **kwargs):\n",
    "    @rl.update_task(as_executable=False)\n",
    "    async def update(*args, **kwargs) -> dict:\n",
    "        model, tokenizer, loaded_from, iteration = load_llama_or_latest_checkpoint(\n",
    "            base_model_id=base_model_id,\n",
    "            lora_id=lora_id,\n",
    "            dtype=torch.bfloat16,\n",
    "        )\n",
    "        model.print_trainable_parameters()\n",
    "        logger.info(\"Model Loaded...\")\n",
    "        training_args = GRPOConfig(\n",
    "            learning_rate = 5e-6,\n",
    "            adam_beta1 = 0.9,\n",
    "            adam_beta2 = 0.99,\n",
    "            weight_decay = 0.1,\n",
    "            warmup_ratio = 0.1,\n",
    "            lr_scheduler_type = \"cosine\",\n",
    "            optim = \"paged_adamw_8bit\",\n",
    "            logging_steps = 1,\n",
    "            generation_batch_size = 8,\n",
    "            per_device_train_batch_size = 1,\n",
    "            gradient_accumulation_steps = 1,\n",
    "            bf16=True,\n",
    "            gradient_checkpointing=False,\n",
    "            num_generations = 8,\n",
    "            max_prompt_length = max_prompt_length,\n",
    "            max_completion_length = max_seq_length,\n",
    "            num_train_epochs = 1,\n",
    "            save_steps = 10,\n",
    "            max_steps = iteration+50,\n",
    "            max_grad_norm = 0.1,\n",
    "            report_to = \"trackio\",\n",
    "            run_name=\"roserun-1\",\n",
    "            output_dir = lora_id,\n",
    "        )\n",
    "        dataset = load_dataset(\"qwedsacf/competition_math\", split=\"train\")\n",
    "        mapped = dataset.map(to_prompt_completion, remove_columns=dataset.column_names).shuffle()\n",
    "        logger.info(\"Configured...\")\n",
    "        trainer = GRPOTrainer(\n",
    "            model = model,\n",
    "            processing_class = tokenizer,\n",
    "            reward_funcs = [\n",
    "                strict_format_reward_func,\n",
    "                correctness_reward_func,\n",
    "            ],\n",
    "            args = training_args,\n",
    "            train_dataset = mapped,\n",
    "        )\n",
    "        logger.info(\"Starting Training...\")\n",
    "        memory_stats()\n",
    "        if loaded_from == base_model_id:\n",
    "            trainer.train(resume_from_checkpoint=False)\n",
    "        else:\n",
    "            trainer.train(resume_from_checkpoint=loaded_from)\n",
    "        logger.info(\"Finished Training...\")\n",
    "        memory_stats()\n",
    "        del model\n",
    "        del tokenizer\n",
    "        del trainer\n",
    "        del dataset\n",
    "        del mapped\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        logger.info(\"Cleaned up memory...\")\n",
    "        memory_stats()\n",
    "    # ========================================================================\n",
    "    # 3. STOP CRITERION TASK\n",
    "    # ========================================================================\n",
    "    @rl.as_stop_criterion(metric_name='MODEL_REWARD', threshold=.5, operator=GREATER_THAN_THRESHOLD, as_executable=False)\n",
    "    async def check_reward(*args, **kwargs):\n",
    "        def rewards_func(prompts, completions, ground_truth, **kwargs):\n",
    "            rewards = []\n",
    "            for prompt, completion, ground in zip(prompts, completions, ground_truth):\n",
    "                c = get_answer(completion)\n",
    "                g = get_answer(ground)\n",
    "                reward = 1.0 if g == c else 0\n",
    "                rewards.append(reward)\n",
    "            return rewards\n",
    "        batch_size=8\n",
    "        iteration=1\n",
    "        model, tokenizer, loaded_from, _ = load_llama_or_latest_checkpoint(\n",
    "            base_model_id=base_model_id,\n",
    "            lora_id=lora_id,\n",
    "            dtype=torch.bfloat16,\n",
    "        )\n",
    "        dataset = load_dataset(\"qwedsacf/competition_math\", split=\"train\")\n",
    "        mapped = dataset.map(to_prompt_completion, remove_columns=dataset.column_names)\n",
    "        total_correct=0.0\n",
    "        logger.info(\"Starting grading...\")\n",
    "        memory_stats()\n",
    "        for step in range(iteration):\n",
    "            shuffled_dataset = mapped.shuffle()\n",
    "            batch = shuffled_dataset.select(range(batch_size))\n",
    "            messages = [\n",
    "                [{\"role\": \"user\", \"content\": ex[\"prompt\"]}]\n",
    "                for ex in batch\n",
    "            ]\n",
    "            inputs = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(model.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    inputs,\n",
    "                    max_new_tokens=2048,\n",
    "                    do_sample=True,      # sampling\n",
    "                    top_p=0.9,\n",
    "                    temperature=0.7,\n",
    "                    pad_token_id = tokenizer.eos_token_id\n",
    "                    # don't usually mix beam search + sampling;\n",
    "                    # if you want beam search, drop top_p/temperature and set num_beams>1\n",
    "                )\n",
    "                \n",
    "            texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            rewards = rewards_func([prompt[\"prompt\"] for prompt in batch], texts, [prompt[\"ground_truth\"] for prompt in batch])\n",
    "            del shuffled_dataset\n",
    "            del batch\n",
    "            del inputs\n",
    "            del outputs\n",
    "            del texts\n",
    "            total_correct = total_correct + sum(rewards)/2\n",
    "        logger.info(\"Finished grading...\")\n",
    "        memory_stats()\n",
    "        del model\n",
    "        del tokenizer\n",
    "        del dataset\n",
    "        del mapped\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        logger.info(\"Cleaned up memory...\")\n",
    "        memory_stats()\n",
    "        return total_correct / (iteration*batch_size)\n",
    "\n",
    "    # Run\n",
    "    logger.info(\"Starting Reinforcement Learning with ROSE...\")\n",
    "    await rl.learn(skip_simulation_step=True,**kwargs)\n",
    "    logger.info(\"Reinforcement Learning completed!\")\n",
    "\n",
    "try:\n",
    "    engine = await ConcurrentExecutionBackend(ProcessPoolExecutor())\n",
    "    asyncflow = await WorkflowEngine.create(engine)\n",
    "    rl = SequentialReinforcementLearner(asyncflow)\n",
    "\n",
    "    init_default_logger(logging.INFO)\n",
    "    await run(rl, max_iter=1)\n",
    "except Exception as e:\n",
    "    print(f'Learner Failed with: {e}')\n",
    "finally:\n",
    "    await rl.shutdown()\n",
    "    logging.getLogger().handlers.clear()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
