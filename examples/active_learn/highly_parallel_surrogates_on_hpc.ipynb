{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb16eae7-efd2-47df-9211-30d22523c8c8",
   "metadata": {},
   "source": [
    "# What is ROSE and why?\n",
    "\n",
    "**ROSE**: RADICAL Orchestrator for Surrogate Exploration (ROSE) toolkit is a framework designed to enable the concurrent and adaptive execution of simulation, surrogate training, and selection tasks on High-Performance Computing (HPC) resources. ROSE allows you to enable, scale, and accelerate your learning workflows across thousands of CPU cores and GPUs effectively and efficiently with just a few lines of code \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"attachment:dfdde6a0-c257-4aa7-bca8-8817ec693214.png\" \n",
    "       width=\"800\" \n",
    "       style=\"border: 3px solid black; border-radius: 10px; margin: 5px;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "* A framework that facilitates building and executing ML learning workflows on different HPC clusters seamlessly.\n",
    "* Offers multiple standardized AL, RL, UQ, and HPO (upcoming) approaches, while also allowing the flexibility to create custom ML workflows.\n",
    "* Built on RCT, leveraging its runtime capabilities to execute heterogeneous tasks on heterogeneous resources.\n",
    "* Open-source and Python-native.\n",
    "\n",
    "\n",
    "### Why ROSE\n",
    "\n",
    "Today’s challenge: Scientists must wire together training, simulation, and learning tasks — this is manual, fragile, not scalable, or performance efficient.\n",
    "\n",
    "ROSE advantage:\n",
    "* Automates, scales, portabilizes, and asynchronously executes workflow learners (training, active learning, UQ, RL) on a large scale on HPC efficently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db84413d-93e9-4715-b4b8-f4d7abada57b",
   "metadata": {},
   "source": [
    "# Standardized Approach for building Learners\n",
    "\n",
    "* Learner is a `unified` workflow that can contain `simulation` --> `training` --> `active learn` steps (tasks) and can be automated.\n",
    "* Learners are often isolated and run asynchronously. ROSE also extends this notion to support learners who can communicate and depend on others accordingly.\n",
    "* Within each learner user can parallelize their `simulation` --> `training` --> `active learn` steps in the best way that fits their resources and needs.\n",
    "* Additionally, users can benefit from ROSE by leveraging learner-level parallelization, allowing them to submit N parallel learners and letting ROSE manage that for them on HPC.\n",
    "\n",
    "## Outlines:\n",
    "This tutorial will teach the user into how to use ROSE to build:\n",
    "* Basic Surrogate Learner.\n",
    "* Parallel Surrogate Learners.\n",
    "* Ensemble Surrogate Learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b1a731-e17c-4db0-a0b6-f7d8a3d09614",
   "metadata": {},
   "source": [
    "## Sequential Learner\n",
    "\n",
    "In this example, we will learn how to use ROSE API to build and submit a `single` Active Learn Learner that either stops when the performance metric threshold is `met` or the number of iterations the user specified is reached (in this case, 5 iterations).\n",
    "\n",
    "```\n",
    "                                                  ┌────────────────────────┐\n",
    "                                                  │      SIMULATION        │\n",
    "                                                  │ (Generate synthetic    │\n",
    "                                                  │  or real-world data)   │\n",
    "                                                  └────────────┬───────────┘\n",
    "                                                               │\n",
    "                                                               ▼\n",
    "                                                  ┌────────────────────────┐\n",
    "                                                  │        TRAINING        │\n",
    "                                                  │ (Train model on        │\n",
    "                                                  │  initial dataset)      │\n",
    "                                                  └────────────┬───────────┘\n",
    "                                                               │\n",
    "                                                               ▼\n",
    "                                                  ┌────────────────────────┐\n",
    "                                                  │     ACTIVE LEARNING    │\n",
    "                                                  │ (Use model to query or │\n",
    "                                                  │  label uncertain data) │\n",
    "                                                  └────────────┬───────────┘\n",
    "                                                               │\n",
    "                                                               ▼\n",
    "                                                  ┌────────────────────────┐\n",
    "                                                  │   IMPROVED MODEL LOOP  │\n",
    "                                                  │(Repeat for N iters     │\n",
    "                                                  │   or performance goal) │\n",
    "                                                  └────────────────────────┘\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ae0aae-91a7-4be9-8ff2-9ffab7b64a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kj/filesystem-disk-unix.c++:1734: warning: PWD environment variable doesn't match current directory; pwd = /home/aymen\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import asyncio\n",
    "import logging\n",
    "\n",
    "from typing import Dict, List, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# learner and task level imports\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ROSE top layer imports\n",
    "from rose.metrics import MEAN_SQUARED_ERROR_MSE\n",
    "from rose.al.active_learner import Learner, SequentialActiveLearner, ParallelActiveLearner\n",
    "\n",
    "# ROSE Bottom layers imports\n",
    "from radical.asyncflow import WorkflowEngine\n",
    "from radical.asyncflow import ConcurrentExecutionBackend\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from radical.asyncflow.logging import init_default_logger\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ebf93-35a7-4915-a632-b314faeeecbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m2025-10-29 18:07:09.604\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[root]\u001b[0m │ Logger configured successfully - Console: INFO, File: disabled (N/A), Structured: disabled, Style: modern\n",
      "\u001b[90m2025-10-29 18:07:09.605\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Starting Online Active Learning with ROSE...\n",
      "Starting Active Learner\n",
      "Starting Iteration-0\n",
      "\u001b[90m2025-10-29 18:07:09.606\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation'] for execution\n",
      "\u001b[90m2025-10-29 18:07:09.607\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating NEW data stream (Iteration 0) ===\n",
      "\u001b[90m2025-10-29 18:07:09.608\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ New data stream: 50 unlabeled samples available\n",
      "\u001b[90m2025-10-29 18:07:09.610\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000001 is in DONE state\n",
      "\u001b[90m2025-10-29 18:07:09.628\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training'] for execution\n",
      "\u001b[90m2025-10-29 18:07:09.628\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 0 ===\n",
      "\u001b[90m2025-10-29 18:07:09.642\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 10, Test RMSE: 0.1437\n",
      "\u001b[90m2025-10-29 18:07:09.644\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000002 is in DONE state\n",
      "\u001b[90m2025-10-29 18:07:09.655\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['active_learn'] for execution\n",
      "\u001b[90m2025-10-29 18:07:09.656\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting from NEW data stream (Iteration 0) ===\n",
      "\u001b[90m2025-10-29 18:07:09.659\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples from new stream\n",
      "\u001b[90m2025-10-29 18:07:09.659\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Total labeled: 10 -> 15\n",
      "\u001b[90m2025-10-29 18:07:09.660\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000003 is in DONE state\n",
      "\u001b[90m2025-10-29 18:07:09.677\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation'] for execution\n",
      "\u001b[90m2025-10-29 18:07:09.677\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating NEW data stream (Iteration 1) ===\n",
      "\u001b[90m2025-10-29 18:07:09.679\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ New data stream: 50 unlabeled samples available\n",
      "\u001b[90m2025-10-29 18:07:09.687\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000004 is in DONE state\n",
      "\u001b[90m2025-10-29 18:07:09.699\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training'] for execution\n",
      "\u001b[90m2025-10-29 18:07:09.700\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 1 ===\n",
      "\u001b[90m2025-10-29 18:07:09.731\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 15, Test RMSE: 0.1101\n",
      "\u001b[90m2025-10-29 18:07:09.734\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000005 is in DONE state\n",
      "Starting Iteration-1\n",
      "\u001b[90m2025-10-29 18:07:09.736\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['active_learn'] for execution\n",
      "\u001b[90m2025-10-29 18:07:09.738\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting from NEW data stream (Iteration 1) ===\n",
      "\u001b[90m2025-10-29 18:07:09.739\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples from new stream\n",
      "\u001b[90m2025-10-29 18:07:09.741\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Total labeled: 15 -> 20\n",
      "\u001b[90m2025-10-29 18:07:09.742\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000006 is in DONE state\n",
      "\u001b[90m2025-10-29 18:07:09.759\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation'] for execution\n",
      "\u001b[90m2025-10-29 18:07:09.759\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating NEW data stream (Iteration 2) ===\n",
      "\u001b[90m2025-10-29 18:07:09.761\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ New data stream: 50 unlabeled samples available\n",
      "\u001b[90m2025-10-29 18:07:09.768\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000007 is in DONE state\n",
      "\u001b[90m2025-10-29 18:07:09.780\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training'] for execution\n",
      "\u001b[90m2025-10-29 18:07:09.781\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 2 ===\n",
      "\u001b[90m2025-10-29 18:07:09.808\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 20, Test RMSE: 0.0754\n",
      "\u001b[90m2025-10-29 18:07:09.811\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000008 is in DONE state\n",
      "Starting Iteration-2\n",
      "\u001b[90m2025-10-29 18:07:09.813\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['active_learn'] for execution\n",
      "\u001b[90m2025-10-29 18:07:09.814\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting from NEW data stream (Iteration 2) ===\n",
      "\u001b[90m2025-10-29 18:07:09.817\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples from new stream\n",
      "\u001b[90m2025-10-29 18:07:09.819\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Total labeled: 20 -> 25\n",
      "\u001b[90m2025-10-29 18:07:09.819\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000009 is in DONE state\n",
      "\u001b[90m2025-10-29 18:07:09.835\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation'] for execution\n",
      "\u001b[90m2025-10-29 18:07:09.836\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating NEW data stream (Iteration 3) ===\n",
      "\u001b[90m2025-10-29 18:07:09.837\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ New data stream: 50 unlabeled samples available\n",
      "\u001b[90m2025-10-29 18:07:09.844\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000010 is in DONE state\n",
      "\u001b[90m2025-10-29 18:07:09.857\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training'] for execution\n",
      "\u001b[90m2025-10-29 18:07:09.858\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 3 ===\n",
      "\u001b[90m2025-10-29 18:07:09.882\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 25, Test RMSE: 0.0784\n",
      "\u001b[90m2025-10-29 18:07:09.884\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000011 is in DONE state\n",
      "Starting Iteration-3\n",
      "\u001b[90m2025-10-29 18:07:09.885\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['active_learn'] for execution\n",
      "\u001b[90m2025-10-29 18:07:09.885\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting from NEW data stream (Iteration 3) ===\n",
      "\u001b[90m2025-10-29 18:07:09.887\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples from new stream\n",
      "\u001b[90m2025-10-29 18:07:09.887\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Total labeled: 25 -> 30\n",
      "\u001b[90m2025-10-29 18:07:09.888\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000012 is in DONE state\n",
      "\u001b[90m2025-10-29 18:07:09.906\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation'] for execution\n",
      "\u001b[90m2025-10-29 18:07:09.907\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating NEW data stream (Iteration 4) ===\n",
      "\u001b[90m2025-10-29 18:07:09.908\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ New data stream: 50 unlabeled samples available\n",
      "\u001b[90m2025-10-29 18:07:09.915\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000013 is in DONE state\n",
      "\u001b[90m2025-10-29 18:07:09.928\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training'] for execution\n",
      "\u001b[90m2025-10-29 18:07:09.929\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 4 ===\n",
      "\u001b[90m2025-10-29 18:07:09.957\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 30, Test RMSE: 0.0784\n",
      "\u001b[90m2025-10-29 18:07:09.959\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000014 is in DONE state\n",
      "Starting Iteration-4\n",
      "\u001b[90m2025-10-29 18:07:09.962\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['active_learn'] for execution\n",
      "\u001b[90m2025-10-29 18:07:09.966\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting from NEW data stream (Iteration 4) ===\n",
      "\u001b[90m2025-10-29 18:07:09.967\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples from new stream\n",
      "\u001b[90m2025-10-29 18:07:09.968\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Total labeled: 30 -> 35\n",
      "\u001b[90m2025-10-29 18:07:09.968\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000015 is in DONE state\n",
      "\u001b[90m2025-10-29 18:07:09.987\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation'] for execution\n",
      "\u001b[90m2025-10-29 18:07:09.987\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating NEW data stream (Iteration 5) ===\n",
      "\u001b[90m2025-10-29 18:07:09.989\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ New data stream: 50 unlabeled samples available\n",
      "\u001b[90m2025-10-29 18:07:09.996\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000016 is in DONE state\n",
      "\u001b[90m2025-10-29 18:07:10.009\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training'] for execution\n",
      "\u001b[90m2025-10-29 18:07:10.010\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 5 ===\n",
      "\u001b[90m2025-10-29 18:07:10.037\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 35, Test RMSE: 0.0685\n",
      "\u001b[90m2025-10-29 18:07:10.039\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000017 is in DONE state\n",
      "\u001b[90m2025-10-29 18:07:10.041\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Online Active Learning completed!\n",
      "\u001b[90m2025-10-29 18:07:10.043\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Initiating shutdown\n",
      "\u001b[90m2025-10-29 18:07:10.044\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[execution.backend(concurrent)]\u001b[0m │ Concurrent execution backend shutdown complete\n",
      "\u001b[90m2025-10-29 18:07:10.046\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Shutdown completed for all components.\n"
     ]
    }
   ],
   "source": [
    "async def run(acl, **kwargs):\n",
    "    INITIAL_SAMPLES = 10\n",
    "    SAMPLES_PER_ITER = 5\n",
    "    POOL_SIZE = 50\n",
    "    \n",
    "    # Shared state for accumulating training data\n",
    "    accumulated_state = {\n",
    "        \"X_train\": None,\n",
    "        \"y_train\": None\n",
    "    }\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 1. SIMULATION TASK\n",
    "    # ========================================================================\n",
    "    @acl.simulation_task(as_executable=False)\n",
    "    async def simulation(*args, **kwargs) -> dict:\n",
    "        # Get iteration from previous active_learn result if it exists\n",
    "        prev_result = args[0] if args else {}\n",
    "        iteration = prev_result.get(\"iteration\", 0)\n",
    "        \n",
    "        logger.info(f\"=== SIMULATION: Generating NEW data stream (Iteration {iteration}) ===\")\n",
    "        \n",
    "        # Use iteration-dependent seed for different data each time\n",
    "        np.random.seed(42 + iteration)\n",
    "        \n",
    "        # Simulate new data arriving\n",
    "        X_new = np.linspace(0, 4 * np.pi, POOL_SIZE).reshape(-1, 1)\n",
    "        y_new = np.sin(X_new).ravel() + np.random.normal(0, 0.1, X_new.shape[0])\n",
    "        \n",
    "        # Test set remains consistent\n",
    "        X_test = np.linspace(0, 4 * np.pi, 100).reshape(-1, 1)\n",
    "        y_test = np.sin(X_test).ravel()\n",
    "        \n",
    "        logger.info(f\"New data stream: {len(X_new)} unlabeled samples available\")\n",
    "        \n",
    "        return {\n",
    "            \"X_new\": X_new,\n",
    "            \"y_new\": y_new,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test,\n",
    "            \"iteration\": iteration\n",
    "        }\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 2. TRAINING TASK\n",
    "    # ========================================================================\n",
    "    @acl.training_task(as_executable=False)\n",
    "    async def training(*args, **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Training depends on: simulation_task\n",
    "        Uses shared accumulated_state for training data\n",
    "        \"\"\"\n",
    "        simulation_result = args[0] if args else kwargs.get(\"data\", {})\n",
    "        iteration = simulation_result.get(\"iteration\", 0)\n",
    "        \n",
    "        logger.info(f\"=== TRAINING: Iteration {iteration} ===\")\n",
    "        \n",
    "        X_test = simulation_result[\"X_test\"]\n",
    "        y_test = simulation_result[\"y_test\"]\n",
    "        \n",
    "        # Use accumulated state\n",
    "        if accumulated_state[\"X_train\"] is None:\n",
    "            # First iteration: start with initial samples\n",
    "            X_new = simulation_result[\"X_new\"]\n",
    "            y_new = simulation_result[\"y_new\"]\n",
    "            \n",
    "            initial_indices = np.random.choice(len(X_new), INITIAL_SAMPLES, replace=False)\n",
    "            X_train = X_new[initial_indices]\n",
    "            y_train = y_new[initial_indices]\n",
    "            \n",
    "            # Store in shared state\n",
    "            accumulated_state[\"X_train\"] = X_train\n",
    "            accumulated_state[\"y_train\"] = y_train\n",
    "        else:\n",
    "            # Use accumulated data from shared state\n",
    "            X_train = accumulated_state[\"X_train\"]\n",
    "            y_train = accumulated_state[\"y_train\"]\n",
    "        \n",
    "        # Train model\n",
    "        kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=0.1)\n",
    "        model = GaussianProcessRegressor(kernel=kernel, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred, y_std = model.predict(X_test, return_std=True)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        logger.info(f\"Training samples: {len(X_train)}, Test RMSE: {rmse:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            \"model\": model,\n",
    "            \"rmse\": rmse,\n",
    "            \"mean_uncertainty\": np.mean(y_std),\n",
    "            \"num_samples\": len(X_train),\n",
    "            \"iteration\": iteration\n",
    "        }\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 3. ACTIVE LEARNING TASK\n",
    "    # ========================================================================\n",
    "    @acl.active_learn_task(as_executable=False)\n",
    "    async def active_learn(*args, **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Active learning depends on: (simulation_task, training_task)\n",
    "        Updates shared accumulated_state with new samples\n",
    "        \"\"\"\n",
    "        simulation_result = args[0]\n",
    "        training_result = args[1]\n",
    "        \n",
    "        iteration = training_result.get(\"iteration\", 0)\n",
    "        \n",
    "        logger.info(f\"=== ACTIVE LEARNING: Selecting from NEW data stream (Iteration {iteration}) ===\")\n",
    "        \n",
    "        model = training_result[\"model\"]\n",
    "        X_new = simulation_result[\"X_new\"]\n",
    "        y_new = simulation_result[\"y_new\"]\n",
    "        \n",
    "        # Predict uncertainty on new unlabeled data\n",
    "        _, y_std = model.predict(X_new, return_std=True)\n",
    "        \n",
    "        # Select top-k most uncertain samples\n",
    "        uncertain_indices = np.argsort(y_std)[-SAMPLES_PER_ITER:]\n",
    "        X_selected = X_new[uncertain_indices]\n",
    "        y_selected = y_new[uncertain_indices]\n",
    "        \n",
    "        # Update shared state\n",
    "        old_size = len(accumulated_state[\"X_train\"])\n",
    "        accumulated_state[\"X_train\"] = np.vstack([accumulated_state[\"X_train\"], X_selected])\n",
    "        accumulated_state[\"y_train\"] = np.concatenate([accumulated_state[\"y_train\"], y_selected])\n",
    "        new_size = len(accumulated_state[\"X_train\"])\n",
    "        \n",
    "        logger.info(f\"Selected {len(X_selected)} uncertain samples from new stream\")\n",
    "        logger.info(f\"Total labeled: {old_size} -> {new_size}\")\n",
    "        \n",
    "        return {\n",
    "            \"iteration\": iteration + 1\n",
    "        }\n",
    "\n",
    "    # Run\n",
    "    logger.info(\"Starting Online Active Learning with ROSE...\")\n",
    "    await acl.start(**kwargs)\n",
    "    logger.info(\"Online Active Learning completed!\")\n",
    "\n",
    "try:\n",
    "    engine = await ConcurrentExecutionBackend(ProcessPoolExecutor())\n",
    "    asyncflow = await WorkflowEngine.create(engine)\n",
    "    acl = SequentialActiveLearner(asyncflow)\n",
    "\n",
    "    init_default_logger(logging.INFO)\n",
    "    await run(acl, max_iter=5)\n",
    "except Exception as e:\n",
    "    print(f'Learner Failed with: {e}')\n",
    "finally:\n",
    "    await acl.shutdown()\n",
    "    logging.getLogger().handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73637a0f-aade-4d8f-b9b7-c0f4ab4ee4f4",
   "metadata": {},
   "source": [
    "#### But what if I want to stop the execution earlier if my performance metric is reached?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df830e3c-9362-47eb-bb38-e1cabaa9beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@acl.as_stop_criterion(metric_name=MEAN_SQUARED_ERROR_MSE, threshold=0.1, as_executable=False)\n",
    "async def check_mse():\n",
    "    \"\"\"\n",
    "    Your stop condition\n",
    "    1-For each AL iteration, check if MSE <= 0.1\n",
    "    2-Terminate early if reached\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d1ce5-50b1-47a4-8697-d4e7812ffae5",
   "metadata": {},
   "source": [
    "## Parallel Learners\n",
    "\n",
    "When we talk about parallel learners, we mean running multiple surrogates at the same time, instead of just one. Different learners can explore different parts of the data space. Running in parallel takes advantage of modern multi-core CPUs or GPUs.\n",
    "\n",
    "```\n",
    "                   SHARED SIMULATION                  |    OWN SIMULATION PER LEARNER\n",
    "                   ───────────────────────────────────|────────────────────────────────────────\n",
    "                           ┌───────────────┐          |        ┌───────────────┐\n",
    "                           │ SIMULATION    │          |        │ SIMULATION 1  │\n",
    "                           │ (Shared)      │          |        │ (Learner 1)   │\n",
    "                           └──────┬────────┘          |        └──────┬────────┘\n",
    "                                  │                   |               │\n",
    "                    ┌─────────────┴─────────────┐     | ┌─────────────┴─────────────┐\n",
    "                    │       TRAINING 1          │     | │          TRAINING 1       │\n",
    "                    │       TRAINING 2          │     | │          TRAINING 2       │\n",
    "                    │       TRAINING N          │     | │          TRAINING N       │\n",
    "                    └─────────────┬─────────────┘     | └─────────────┬─────────────┘\n",
    "                                  │                   |               │\n",
    "                    ┌─────────────┴─────────────┐     | ┌─────────────┴─────────────┐\n",
    "                    │   ACTIVE LEARNING 1..N    │     | │     ACTIVE LEARNING 1     │\n",
    "                    └─────────────┬─────────────┘     | │     ACTIVE LEARNING 2     │\n",
    "                                  │                   | │     ACTIVE LEARNING N     │\n",
    "                    ┌─────────────┴─────────────┐     | └─────────────┬─────────────┘\n",
    "                    │   IMPROVED LOOP 1..N      │     | ┌─────────────┴─────────────┐\n",
    "                    └───────────────────────────┘     | │     IMPROVED LOOP 1       │\n",
    "                                                      | │     IMPROVED LOOP 2       │\n",
    "                                                      | │     IMPROVED LOOP N       │\n",
    "                                                      | └───────────────────────────┘\n",
    "                               \n",
    "\n",
    "```\n",
    "\n",
    "In ROSE, scaling your Learners on HPC is done by using the `Parallel Learners` instead of the `Sequential Learners`. \n",
    "\n",
    "The example below will show how to build and launch 2 truly asynchronous and parallel learners on your compute resources in a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51bf4738-8472-4500-be2c-fc798d746891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m2025-10-29 18:12:00.390\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[root]\u001b[0m │ Logger configured successfully - Console: INFO, File: disabled (N/A), Structured: disabled, Style: modern\n",
      "\u001b[90m2025-10-29 18:12:00.392\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Starting Online Active Learning with ROSE...\n",
      "Starting Parallel Active Learning with 2 learners\n",
      "Starting Active Learner (Learner-0)\n",
      "[Learner-0] Starting Iteration-0\n",
      "Starting Active Learner (Learner-1)\n",
      "[Learner-1] Starting Iteration-0\n",
      "\u001b[90m2025-10-29 18:12:00.394\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation', 'simulation'] for execution\n",
      "\u001b[90m2025-10-29 18:12:00.394\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating NEW data stream (Iteration 0) ===\n",
      "\u001b[90m2025-10-29 18:12:00.395\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ New data stream: 50 unlabeled samples available\n",
      "\u001b[90m2025-10-29 18:12:00.398\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000001 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.398\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating NEW data stream (Iteration 0) ===\n",
      "\u001b[90m2025-10-29 18:12:00.399\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ New data stream: 50 unlabeled samples available\n",
      "\u001b[90m2025-10-29 18:12:00.402\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000006 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.415\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training', 'training'] for execution\n",
      "\u001b[90m2025-10-29 18:12:00.416\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 0 ===\n",
      "\u001b[90m2025-10-29 18:12:00.449\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 10, Test RMSE: 0.1437\n",
      "\u001b[90m2025-10-29 18:12:00.452\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000002 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.453\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 0 ===\n",
      "\u001b[90m2025-10-29 18:12:00.478\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 10, Test RMSE: 0.1437\n",
      "\u001b[90m2025-10-29 18:12:00.480\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000007 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.491\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['active_learn', 'active_learn'] for execution\n",
      "\u001b[90m2025-10-29 18:12:00.492\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting from NEW data stream (Iteration 0) ===\n",
      "\u001b[90m2025-10-29 18:12:00.493\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples from new stream\n",
      "\u001b[90m2025-10-29 18:12:00.496\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Total labeled: 10 -> 15\n",
      "\u001b[90m2025-10-29 18:12:00.496\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000003 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.497\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting from NEW data stream (Iteration 0) ===\n",
      "\u001b[90m2025-10-29 18:12:00.500\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples from new stream\n",
      "\u001b[90m2025-10-29 18:12:00.501\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Total labeled: 15 -> 20\n",
      "\u001b[90m2025-10-29 18:12:00.502\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000008 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.514\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation', 'simulation'] for execution\n",
      "\u001b[90m2025-10-29 18:12:00.515\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating NEW data stream (Iteration 1) ===\n",
      "\u001b[90m2025-10-29 18:12:00.517\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ New data stream: 50 unlabeled samples available\n",
      "\u001b[90m2025-10-29 18:12:00.522\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000004 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.524\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating NEW data stream (Iteration 1) ===\n",
      "\u001b[90m2025-10-29 18:12:00.525\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ New data stream: 50 unlabeled samples available\n",
      "\u001b[90m2025-10-29 18:12:00.532\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000009 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.543\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training', 'training'] for execution\n",
      "\u001b[90m2025-10-29 18:12:00.545\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 1 ===\n",
      "\u001b[90m2025-10-29 18:12:00.579\u001b[0m │ \u001b[93mWARNING\u001b[0m │ \u001b[38;5;165m[warnings]\u001b[0m │ /home/aymen/ve/rct_debug_latest/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[90m2025-10-29 18:12:00.581\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 20, Test RMSE: 0.6655\n",
      "\u001b[90m2025-10-29 18:12:00.583\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000005 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.584\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 1 ===\n",
      "\u001b[90m2025-10-29 18:12:00.599\u001b[0m │ \u001b[93mWARNING\u001b[0m │ \u001b[38;5;165m[warnings]\u001b[0m │ /home/aymen/ve/rct_debug_latest/lib/python3.12/site-packages/sklearn/gaussian_process/kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "\n",
      "\u001b[90m2025-10-29 18:12:00.602\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 20, Test RMSE: 0.6655\n",
      "\u001b[90m2025-10-29 18:12:00.604\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000010 is in DONE state\n",
      "[Learner-0] Starting Iteration-1\n",
      "[Learner-1] Starting Iteration-1\n",
      "\u001b[90m2025-10-29 18:12:00.606\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['active_learn', 'active_learn'] for execution\n",
      "\u001b[90m2025-10-29 18:12:00.607\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting from NEW data stream (Iteration 1) ===\n",
      "\u001b[90m2025-10-29 18:12:00.609\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples from new stream\n",
      "\u001b[90m2025-10-29 18:12:00.610\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Total labeled: 20 -> 25\n",
      "\u001b[90m2025-10-29 18:12:00.610\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000011 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.611\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting from NEW data stream (Iteration 1) ===\n",
      "\u001b[90m2025-10-29 18:12:00.613\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples from new stream\n",
      "\u001b[90m2025-10-29 18:12:00.614\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Total labeled: 25 -> 30\n",
      "\u001b[90m2025-10-29 18:12:00.614\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000014 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.628\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation', 'simulation'] for execution\n",
      "\u001b[90m2025-10-29 18:12:00.629\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating NEW data stream (Iteration 2) ===\n",
      "\u001b[90m2025-10-29 18:12:00.630\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ New data stream: 50 unlabeled samples available\n",
      "\u001b[90m2025-10-29 18:12:00.636\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000012 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.637\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating NEW data stream (Iteration 2) ===\n",
      "\u001b[90m2025-10-29 18:12:00.637\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ New data stream: 50 unlabeled samples available\n",
      "\u001b[90m2025-10-29 18:12:00.643\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000015 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.655\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training', 'training'] for execution\n",
      "\u001b[90m2025-10-29 18:12:00.656\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 2 ===\n",
      "\u001b[90m2025-10-29 18:12:00.680\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 30, Test RMSE: 0.0779\n",
      "\u001b[90m2025-10-29 18:12:00.681\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000013 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.682\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 2 ===\n",
      "\u001b[90m2025-10-29 18:12:00.706\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 30, Test RMSE: 0.0779\n",
      "\u001b[90m2025-10-29 18:12:00.708\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000016 is in DONE state\n",
      "[Learner-0] Starting Iteration-2\n",
      "[Learner-1] Starting Iteration-2\n",
      "\u001b[90m2025-10-29 18:12:00.710\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['active_learn', 'active_learn'] for execution\n",
      "\u001b[90m2025-10-29 18:12:00.711\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting from NEW data stream (Iteration 2) ===\n",
      "\u001b[90m2025-10-29 18:12:00.713\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples from new stream\n",
      "\u001b[90m2025-10-29 18:12:00.713\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Total labeled: 30 -> 35\n",
      "\u001b[90m2025-10-29 18:12:00.714\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000017 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.715\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting from NEW data stream (Iteration 2) ===\n",
      "\u001b[90m2025-10-29 18:12:00.717\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples from new stream\n",
      "\u001b[90m2025-10-29 18:12:00.719\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Total labeled: 35 -> 40\n",
      "\u001b[90m2025-10-29 18:12:00.719\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000020 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.731\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation', 'simulation'] for execution\n",
      "\u001b[90m2025-10-29 18:12:00.733\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating NEW data stream (Iteration 3) ===\n",
      "\u001b[90m2025-10-29 18:12:00.734\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ New data stream: 50 unlabeled samples available\n",
      "\u001b[90m2025-10-29 18:12:00.741\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000018 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.742\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating NEW data stream (Iteration 3) ===\n",
      "\u001b[90m2025-10-29 18:12:00.744\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ New data stream: 50 unlabeled samples available\n",
      "\u001b[90m2025-10-29 18:12:00.749\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000021 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.763\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training', 'training'] for execution\n",
      "\u001b[90m2025-10-29 18:12:00.764\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 3 ===\n",
      "\u001b[90m2025-10-29 18:12:00.790\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 40, Test RMSE: 0.0535\n",
      "\u001b[90m2025-10-29 18:12:00.793\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000019 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.795\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 3 ===\n",
      "\u001b[90m2025-10-29 18:12:00.821\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 40, Test RMSE: 0.0535\n",
      "\u001b[90m2025-10-29 18:12:00.823\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000022 is in DONE state\n",
      "\u001b[90m2025-10-29 18:12:00.825\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Online Active Learning completed!\n",
      "\u001b[90m2025-10-29 18:12:00.827\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Initiating shutdown\n",
      "\u001b[90m2025-10-29 18:12:00.829\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[execution.backend(concurrent)]\u001b[0m │ Concurrent execution backend shutdown complete\n",
      "\u001b[90m2025-10-29 18:12:00.830\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Shutdown completed for all components.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    engine = await ConcurrentExecutionBackend(ProcessPoolExecutor())\n",
    "    asyncflow = await WorkflowEngine.create(engine)\n",
    "    acl = ParallelActiveLearner(asyncflow)\n",
    "\n",
    "    init_default_logger(logging.INFO)\n",
    "    await run(acl, max_iter=3, parallel_learners=2)\n",
    "except Exception as e:\n",
    "    print(f'Learner Failed with: {e}')\n",
    "finally:\n",
    "    await acl.shutdown()\n",
    "    logging.getLogger().handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255e4bc-fb2a-472a-8761-8a1d6bf4f964",
   "metadata": {},
   "source": [
    "### Building Ensembe Surrogates with ROSE\n",
    "When we talk about ensemble surrogates, we mean training multiple models at the same time, each with different hyperparameters, instead of one after another. Different models explore different parts of the parameter space, giving us better predictions and uncertainty estimates. In ROSE, scaling your ensemble training to 100+ models is done by submitting in parallel, leveraging the higher-level parallelism of learners and tasks. \n",
    "\n",
    "```\n",
    "ENSEMBLE SURROGATE WORKFLOW (ROSE)\n",
    "===================================\n",
    "\n",
    "                    ┌─────────────────┐\n",
    "                    │  Training Data  │\n",
    "                    │  (X, y)         │\n",
    "                    └────────┬────────┘\n",
    "                             │\n",
    "                             ▼\n",
    "              ┌──────────────────────────────┐\n",
    "              │  Generate N Configurations   │\n",
    "              │  (diverse hyperparameters)   │\n",
    "              └──────────────┬───────────────┘\n",
    "                             │\n",
    "        ┌────────────────────┼────────────────────┐\n",
    "        │                    │                    │\n",
    "        ▼                    ▼                    ▼\n",
    "   ┌─────────┐          ┌─────────┐          ┌─────────┐\n",
    "   │ Config1 │          │ Config2 │   ...    │ ConfigN │\n",
    "   │ ls=0.5  │          │ ls=1.2  │          │ ls=1.8  │\n",
    "   └────┬────┘          └────┬────┘          └────┬────┘\n",
    "        │                    │                    │\n",
    "        ▼                    ▼                    ▼\n",
    "   ╔═════════╗          ╔═════════╗          ╔═════════╗\n",
    "   ║ TRAIN 1 ║          ║ TRAIN 2 ║   ...    ║ TRAIN N ║  ← PARALLEL\n",
    "   ║ (async) ║          ║ (async) ║          ║ (async) ║\n",
    "   ╚════┬════╝          ╚════┬════╝          ╚════┬════╝\n",
    "        │                    │                    │\n",
    "        └────────────────────┼────────────────────┘\n",
    "                             │\n",
    "                             ▼\n",
    "                    ┌─────────────────┐\n",
    "                    │ Ensemble Combine│\n",
    "                    │  • Average pred │\n",
    "                    │  • Uncertainty  │\n",
    "                    └────────┬────────┘\n",
    "                             │\n",
    "                             ▼\n",
    "                    ┌─────────────────┐\n",
    "                    │ Robust Surrogate│\n",
    "                    │ Ready for use!  │\n",
    "                    └─────────────────┘\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "The example below will show how to build and train a robust ensemble of surrogate models using ROSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a6e36c7-f785-4cd7-b7e0-25115be17a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m2025-10-29 19:21:22.286\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[root]\u001b[0m │ Logger configured successfully - Console: INFO, File: disabled (N/A), Structured: disabled, Style: modern\n",
      "\u001b[90m2025-10-29 19:21:22.288\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[execution.backend(concurrent)]\u001b[0m │ ProcessPoolExecutor execution backend started successfully\n",
      "\u001b[90m2025-10-29 19:21:22.291\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Generating synthetic stellarator geometry data...\n",
      "\u001b[90m2025-10-29 19:21:22.292\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ ======================================================================\n",
      "\u001b[90m2025-10-29 19:21:22.293\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training 10 surrogate models in parallel...\n",
      "\u001b[90m2025-10-29 19:21:22.294\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ ======================================================================\n",
      "\u001b[90m2025-10-29 19:21:22.294\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Generated 10 diverse ensemble configurations\n",
      "\u001b[90m2025-10-29 19:21:22.295\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Submitted 10 parallel surrogate training tasks\n",
      "\u001b[90m2025-10-29 19:21:22.297\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train'] for execution\n",
      "\u001b[90m2025-10-29 19:21:22.298\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 0] Starting training with config: RBF_0\n",
      "\u001b[90m2025-10-29 19:21:22.563\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 0] Completed in 0.26s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-29 19:21:22.565\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000001 is in DONE state\n",
      "\u001b[90m2025-10-29 19:21:22.566\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 1] Starting training with config: RBF_1\n",
      "\u001b[90m2025-10-29 19:21:22.875\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 1] Completed in 0.31s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-29 19:21:22.877\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000002 is in DONE state\n",
      "\u001b[90m2025-10-29 19:21:22.878\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 2] Starting training with config: RBF_2\n",
      "\u001b[90m2025-10-29 19:21:23.157\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 2] Completed in 0.28s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-29 19:21:23.158\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000003 is in DONE state\n",
      "\u001b[90m2025-10-29 19:21:23.160\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 3] Starting training with config: RBF_3\n",
      "\u001b[90m2025-10-29 19:21:23.544\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 3] Completed in 0.38s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-29 19:21:23.546\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000004 is in DONE state\n",
      "\u001b[90m2025-10-29 19:21:23.547\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 4] Starting training with config: RBF_4\n",
      "\u001b[90m2025-10-29 19:21:23.795\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 4] Completed in 0.25s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-29 19:21:23.797\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000005 is in DONE state\n",
      "\u001b[90m2025-10-29 19:21:23.798\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 5] Starting training with config: RBF_5\n",
      "\u001b[90m2025-10-29 19:21:24.231\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 5] Completed in 0.43s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-29 19:21:24.233\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000006 is in DONE state\n",
      "\u001b[90m2025-10-29 19:21:24.234\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 6] Starting training with config: RBF_6\n",
      "\u001b[90m2025-10-29 19:21:24.489\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 6] Completed in 0.25s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-29 19:21:24.491\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000007 is in DONE state\n",
      "\u001b[90m2025-10-29 19:21:24.492\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 7] Starting training with config: RBF_7\n",
      "\u001b[90m2025-10-29 19:21:24.713\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 7] Completed in 0.22s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-29 19:21:24.714\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000008 is in DONE state\n",
      "\u001b[90m2025-10-29 19:21:24.716\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 8] Starting training with config: RBF_8\n",
      "\u001b[90m2025-10-29 19:21:25.010\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 8] Completed in 0.29s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-29 19:21:25.012\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000009 is in DONE state\n",
      "\u001b[90m2025-10-29 19:21:25.013\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 9] Starting training with config: RBF_9\n",
      "\u001b[90m2025-10-29 19:21:25.214\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 9] Completed in 0.20s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-29 19:21:25.215\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000010 is in DONE state\n",
      "\u001b[90m2025-10-29 19:21:25.217\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Completed: 10/10 models\n",
      "\u001b[90m2025-10-29 19:21:25.227\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['ensemble_predict'] for execution\n",
      "\u001b[90m2025-10-29 19:21:25.228\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Combining predictions from 10 models\n",
      "\u001b[90m2025-10-29 19:21:25.232\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000011 is in DONE state\n",
      "\u001b[90m2025-10-29 19:21:25.234\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ ======================================================================\n",
      "\u001b[90m2025-10-29 19:21:25.235\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ ENSEMBLE TRAINING COMPLETE\n",
      "\u001b[90m2025-10-29 19:21:25.236\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ ======================================================================\n",
      "\u001b[90m2025-10-29 19:21:25.237\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Successfully trained: 10 models\n",
      "\u001b[90m2025-10-29 19:21:25.240\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Average time per model: 0.29s\n",
      "\u001b[90m2025-10-29 19:21:25.241\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Total parallel time: 0.43s\n",
      "\u001b[90m2025-10-29 19:21:25.242\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Ensemble Statistics:\n",
      "\u001b[90m2025-10-29 19:21:25.244\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Best single model RMSE: 0.1602\n",
      "\u001b[90m2025-10-29 19:21:25.246\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Ensemble diversity: 0.0000\n",
      "\u001b[90m2025-10-29 19:21:25.247\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Mean uncertainty: 0.1993\n",
      "\u001b[90m2025-10-29 19:21:25.249\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Initiating shutdown\n",
      "\u001b[90m2025-10-29 19:21:25.251\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[execution.backend(concurrent)]\u001b[0m │ Concurrent execution backend shutdown complete\n",
      "\u001b[90m2025-10-29 19:21:25.253\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Shutdown completed for all components.\n"
     ]
    }
   ],
   "source": [
    "init_default_logger(logging.INFO)\n",
    "\n",
    "# Setup backend and workflow engine\n",
    "engine = await ConcurrentExecutionBackend(ProcessPoolExecutor(max_workers=10))\n",
    "asyncflow = await WorkflowEngine.create(engine)\n",
    "learner = Learner(asyncflow)\n",
    "\n",
    "@learner.training_task(as_executable=False)\n",
    "async def train(*args, **kwargs) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Train a single surrogate model\n",
    "    This represents training ONE of the 100+ models in the ensemble\n",
    "    \"\"\"\n",
    "    \n",
    "    model_id = kwargs.get(\"model_id\", 0)\n",
    "    config = kwargs.get(\"config\", {})\n",
    "    train_data = kwargs.get(\"train_data\")\n",
    "    \n",
    "    logger.info(f\"[Model {model_id}] Starting training with config: {config['kernel']}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    X_train = train_data[\"X\"]\n",
    "    y_train = train_data[\"y\"]\n",
    "    X_test = train_data[\"X_test\"]\n",
    "    \n",
    "    # Train surrogate with specific configuration\n",
    "    kernel = RBF(length_scale=config[\"length_scale\"]) + \\\n",
    "             WhiteKernel(noise_level=config[\"noise_level\"])\n",
    "    \n",
    "    model = GaussianProcessRegressor(\n",
    "        kernel=kernel,\n",
    "        n_restarts_optimizer=config.get(\"n_restarts\", 10)\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict with uncertainty\n",
    "    y_pred, y_std = model.predict(X_test, return_std=True)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    y_test = train_data[\"y_test\"]\n",
    "    rmse = np.sqrt(np.mean((y_pred - y_test) ** 2))\n",
    "    \n",
    "    logger.info(f\"[Model {model_id}] Completed in {training_time:.2f}s, RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"model_id\": model_id,\n",
    "        \"model_type\": config[\"model_type\"],\n",
    "        \"kernel_config\": config[\"kernel\"],\n",
    "        \"rmse\": rmse,\n",
    "        \"training_time\": training_time,\n",
    "        \"predictions\": y_pred,\n",
    "        \"uncertainties\": y_std,\n",
    "        \"success\": True\n",
    "    }\n",
    "\n",
    "@learner.utility_task(as_executable=False)\n",
    "async def ensemble_predict(*args, **kwargs) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Combine predictions from an ensemble of surrogates\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    surrogate_results = kwargs.get(\"surrogate_results\", [])\n",
    "    \n",
    "    logger.info(f\"Combining predictions from {len(surrogate_results)} models\")\n",
    "    \n",
    "    # Extract predictions and uncertainties\n",
    "    all_predictions = [r[\"predictions\"] for r in surrogate_results]\n",
    "    all_uncertainties = [r[\"uncertainties\"] for r in surrogate_results]\n",
    "    \n",
    "    # Ensemble prediction strategies\n",
    "    \n",
    "    # 1. Simple averaging\n",
    "    ensemble_mean = np.mean(all_predictions, axis=0)\n",
    "    \n",
    "    # 2. Uncertainty-weighted averaging\n",
    "    weights = 1.0 / (np.array([r[\"rmse\"] for r in surrogate_results]) + 1e-10)\n",
    "    weights = weights / np.sum(weights)\n",
    "    ensemble_weighted = np.average(all_predictions, axis=0, weights=weights)\n",
    "    \n",
    "    # 3. Ensemble uncertainty (variance across models)\n",
    "    ensemble_variance = np.var(all_predictions, axis=0)\n",
    "    ensemble_std = np.sqrt(ensemble_variance)\n",
    "    \n",
    "    # 4. Combined uncertainty (aleatoric + epistemic)\n",
    "    mean_aleatoric = np.mean(all_uncertainties, axis=0)\n",
    "    total_uncertainty = np.sqrt(ensemble_variance + mean_aleatoric**2)\n",
    "    \n",
    "    return {\n",
    "        \"n_models\": len(surrogate_results),\n",
    "        \"ensemble_mean\": ensemble_mean,\n",
    "        \"ensemble_weighted\": ensemble_weighted,\n",
    "        \"ensemble_std\": ensemble_std,\n",
    "        \"total_uncertainty\": total_uncertainty,\n",
    "        \"individual_rmse\": [r[\"rmse\"] for r in surrogate_results],\n",
    "        \"best_model_rmse\": min(r[\"rmse\"] for r in surrogate_results),\n",
    "        \"ensemble_diversity\": float(np.mean(ensemble_variance))\n",
    "    }\n",
    "\n",
    "# run locally so we will not decorate it\n",
    "def generate(n_models: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate diverse configurations for ensemble models\n",
    "    \n",
    "    Key idea: Diversity in the ensemble for better uncertainty quantification\n",
    "    \"\"\"\n",
    "    configs = []\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        config = {\n",
    "            \"model_id\": i,\n",
    "            \"model_type\": \"gpr\",\n",
    "            \"kernel\": f\"RBF_{i}\",\n",
    "            \"length_scale\": np.random.uniform(0.1, 2.0),\n",
    "            \"noise_level\": np.random.uniform(0.001, 0.1),\n",
    "            \"n_restarts\": 10\n",
    "        }\n",
    "        configs.append(config)\n",
    "    \n",
    "    logger.info(f\"Generated {len(configs)} diverse ensemble configurations\")\n",
    "    return configs\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Train an ensemble of N surrogate models in parallel using ROSE\n",
    "    \"\"\"\n",
    "    # Generate synthetic training data (replace with stellarator data)\n",
    "    logger.info(\"Generating synthetic stellarator geometry data...\")\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    n_train = 100\n",
    "    n_test = 20\n",
    "    n_features = 5\n",
    "    \n",
    "    X_train = np.random.randn(n_train, n_features)\n",
    "    y_train = np.sin(X_train[:, 0]) + 0.5 * np.cos(X_train[:, 1]) + \\\n",
    "              np.random.randn(n_train) * 0.1\n",
    "    \n",
    "    X_test = np.random.randn(n_test, n_features)\n",
    "    y_test = np.sin(X_test[:, 0]) + 0.5 * np.cos(X_test[:, 1]) + \\\n",
    "             np.random.randn(n_test) * 0.1\n",
    "\n",
    "    train_data = {\n",
    "        \"X\": X_train,\n",
    "        \"y\": y_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "    \n",
    "    # Main workflow: Train ensemble\n",
    "    async def train_ensemble():\n",
    "        n_ensemble_models = 10\n",
    "        \n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(f\"Training {n_ensemble_models} surrogate models in parallel...\")\n",
    "        logger.info(\"=\"*70)\n",
    "        \n",
    "        # Generate diverse configurations\n",
    "        configs = generate(n_ensemble_models)\n",
    "        \n",
    "        # PARALLEL TRAINING: Submit all surrogate training tasks\n",
    "        training_tasks = []\n",
    "        for config in configs:\n",
    "            # Each call returns a future - they will run in parallel!\n",
    "            task = train(\n",
    "                model_id=config[\"model_id\"],\n",
    "                config=config,\n",
    "                train_data=train_data\n",
    "            )\n",
    "            training_tasks.append(task)\n",
    "        \n",
    "        logger.info(f\"Submitted {len(training_tasks)} parallel surrogate training tasks\")\n",
    "        \n",
    "        # Wait for all models to complete\n",
    "        surrogate_results = await asyncio.gather(*training_tasks)\n",
    "        \n",
    "        # Filter successful models\n",
    "        successful_results = [r for r in surrogate_results if r.get(\"success\", False)]\n",
    "        \n",
    "        logger.info(f\"Completed: {len(successful_results)}/{n_ensemble_models} models\")\n",
    "        \n",
    "        # Combine into ensemble\n",
    "        ensemble_result = await ensemble_predict(surrogate_results=successful_results)\n",
    "        \n",
    "        # Results\n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(\"ENSEMBLE TRAINING COMPLETE\")\n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(f\"Successfully trained: {len(successful_results)} models\")\n",
    "        \n",
    "        training_times = [r[\"training_time\"] for r in successful_results]\n",
    "        logger.info(f\"Average time per model: {np.mean(training_times):.2f}s\")\n",
    "        logger.info(f\"Total parallel time: {max(training_times):.2f}s\")\n",
    "        \n",
    "        logger.info(f\"Ensemble Statistics:\")\n",
    "        logger.info(f\"Best single model RMSE: {ensemble_result['best_model_rmse']:.4f}\")\n",
    "        logger.info(f\"Ensemble diversity: {ensemble_result['ensemble_diversity']:.4f}\")\n",
    "        logger.info(f\"Mean uncertainty: {np.mean(ensemble_result['total_uncertainty']):.4f}\")\n",
    "        \n",
    "        return ensemble_result\n",
    "\n",
    "    await train_ensemble()\n",
    "\n",
    "    # Cleanup\n",
    "    await learner.shutdown()\n",
    "    logging.getLogger().handlers.clear()\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
