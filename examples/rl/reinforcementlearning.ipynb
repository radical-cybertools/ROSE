{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA0MnVIPk8ha"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb\n",
        "!pip install pyvirtualdisplay"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUypSGvCor1G",
        "outputId": "bc2eafe6-3157-460d-ac1b-06066c978a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 7,814 kB of archives.\n",
            "After this operation, 12.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.13 [29.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.13 [863 kB]\n",
            "Fetched 7,814 kB in 0s (16.1 MB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 124947 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.13_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.13) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.13_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.13) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.13) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.13) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ],
      "metadata": {
        "id": "wklMqcpooynp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xGfC9eGo0Bl",
        "outputId": "734e5df3-f9ef-45a9-abe1-d21ac2efb5a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7d1b3b9622d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "GAMMA = 0.99\n",
        "LR = 1e-3\n",
        "BATCH_SIZE = 64\n",
        "MEMORY_SIZE = 10_000\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_END = 0.01\n",
        "EPSILON_DECAY = 0.995\n",
        "TARGET_UPDATE = 10\n",
        "EPISODES = 100"
      ],
      "metadata": {
        "id": "C0oyRW0HlX7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Neural Network for DQN\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "ct-55lzxla79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experience Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        samples = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*samples)\n",
        "        return (np.array(states), np.array(actions), np.array(rewards, dtype=np.float32),\n",
        "                np.array(next_states), np.array(dones, dtype=np.float32))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "Q3Nz81oSleue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Epsilon-greedy policy\n",
        "def select_action(state, policy_net, epsilon, action_dim):\n",
        "    if random.random() < epsilon:\n",
        "        return random.randint(0, action_dim - 1)\n",
        "    else:\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            return torch.argmax(policy_net(state)).item()"
      ],
      "metadata": {
        "id": "HfPxN9KRlgKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_dqn():\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    policy_net = DQN(state_dim, action_dim)\n",
        "    target_net = DQN(state_dim, action_dim)\n",
        "    target_net.load_state_dict(policy_net.state_dict())\n",
        "    target_net.eval()\n",
        "\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
        "    memory = ReplayBuffer(MEMORY_SIZE)\n",
        "    epsilon = EPSILON_START\n",
        "    rewards_list = []\n",
        "\n",
        "    for episode in range(EPISODES):\n",
        "        state, _ = env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = select_action(state, policy_net, epsilon, action_dim)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            memory.push(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            if len(memory) >= BATCH_SIZE:\n",
        "                # Sample mini-batch\n",
        "                states, actions, rewards, next_states, dones = memory.sample(BATCH_SIZE)\n",
        "\n",
        "                states = torch.tensor(states, dtype=torch.float32)\n",
        "                actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
        "                rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "                next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "                dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "                # Compute Q-values\n",
        "                q_values = policy_net(states).gather(1, actions).squeeze()\n",
        "                with torch.no_grad():\n",
        "                    next_q_values = target_net(next_states).max(1)[0]\n",
        "                    target_q_values = rewards + GAMMA * next_q_values * (1 - dones)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = nn.MSELoss()(q_values, target_q_values)\n",
        "\n",
        "                # Optimize the model\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        rewards_list.append(total_reward)\n",
        "        epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)\n",
        "\n",
        "        # Update target network\n",
        "        if episode % TARGET_UPDATE == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        print(f\"Episode {episode+1}, Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "    torch.save(policy_net.state_dict(), \"dqn_cartpole.pth\")"
      ],
      "metadata": {
        "id": "7Qeh2J5am4wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def watch_trained_agent():\n",
        "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    policy_net = DQN(state_dim, action_dim)\n",
        "    policy_net.load_state_dict(torch.load(\"dqn_cartpole.pth\"))\n",
        "    policy_net.eval()\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    prev_screen = env.render()\n",
        "    plt.imshow(prev_screen)\n",
        "\n",
        "    while True:\n",
        "        screen = env.render()\n",
        "        plt.imshow(screen)\n",
        "        ipythondisplay.clear_output(wait=True)\n",
        "        ipythondisplay.display(plt.gcf())\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            action = torch.argmax(policy_net(state_tensor)).item()\n",
        "\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    env.close()\n",
        "    print(f\"Total reward: {total_reward}\")"
      ],
      "metadata": {
        "id": "cJalvW6lnzJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dqn()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5znQojKm-sZ",
        "outputId": "4bab532e-6c09-4e7f-af05-ef1e9ad2ec8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Reward: 33.0, Epsilon: 0.995\n",
            "Episode 2, Reward: 34.0, Epsilon: 0.990\n",
            "Episode 3, Reward: 27.0, Epsilon: 0.985\n",
            "Episode 4, Reward: 16.0, Epsilon: 0.980\n",
            "Episode 5, Reward: 21.0, Epsilon: 0.975\n",
            "Episode 6, Reward: 19.0, Epsilon: 0.970\n",
            "Episode 7, Reward: 17.0, Epsilon: 0.966\n",
            "Episode 8, Reward: 8.0, Epsilon: 0.961\n",
            "Episode 9, Reward: 11.0, Epsilon: 0.956\n",
            "Episode 10, Reward: 10.0, Epsilon: 0.951\n",
            "Episode 11, Reward: 24.0, Epsilon: 0.946\n",
            "Episode 12, Reward: 28.0, Epsilon: 0.942\n",
            "Episode 13, Reward: 18.0, Epsilon: 0.937\n",
            "Episode 14, Reward: 16.0, Epsilon: 0.932\n",
            "Episode 15, Reward: 14.0, Epsilon: 0.928\n",
            "Episode 16, Reward: 24.0, Epsilon: 0.923\n",
            "Episode 17, Reward: 13.0, Epsilon: 0.918\n",
            "Episode 18, Reward: 41.0, Epsilon: 0.914\n",
            "Episode 19, Reward: 14.0, Epsilon: 0.909\n",
            "Episode 20, Reward: 17.0, Epsilon: 0.905\n",
            "Episode 21, Reward: 15.0, Epsilon: 0.900\n",
            "Episode 22, Reward: 19.0, Epsilon: 0.896\n",
            "Episode 23, Reward: 28.0, Epsilon: 0.891\n",
            "Episode 24, Reward: 20.0, Epsilon: 0.887\n",
            "Episode 25, Reward: 27.0, Epsilon: 0.882\n",
            "Episode 26, Reward: 20.0, Epsilon: 0.878\n",
            "Episode 27, Reward: 15.0, Epsilon: 0.873\n",
            "Episode 28, Reward: 43.0, Epsilon: 0.869\n",
            "Episode 29, Reward: 23.0, Epsilon: 0.865\n",
            "Episode 30, Reward: 17.0, Epsilon: 0.860\n",
            "Episode 31, Reward: 17.0, Epsilon: 0.856\n",
            "Episode 32, Reward: 88.0, Epsilon: 0.852\n",
            "Episode 33, Reward: 22.0, Epsilon: 0.848\n",
            "Episode 34, Reward: 14.0, Epsilon: 0.843\n",
            "Episode 35, Reward: 22.0, Epsilon: 0.839\n",
            "Episode 36, Reward: 54.0, Epsilon: 0.835\n",
            "Episode 37, Reward: 14.0, Epsilon: 0.831\n",
            "Episode 38, Reward: 14.0, Epsilon: 0.827\n",
            "Episode 39, Reward: 24.0, Epsilon: 0.822\n",
            "Episode 40, Reward: 27.0, Epsilon: 0.818\n",
            "Episode 41, Reward: 62.0, Epsilon: 0.814\n",
            "Episode 42, Reward: 36.0, Epsilon: 0.810\n",
            "Episode 43, Reward: 45.0, Epsilon: 0.806\n",
            "Episode 44, Reward: 24.0, Epsilon: 0.802\n",
            "Episode 45, Reward: 32.0, Epsilon: 0.798\n",
            "Episode 46, Reward: 85.0, Epsilon: 0.794\n",
            "Episode 47, Reward: 18.0, Epsilon: 0.790\n",
            "Episode 48, Reward: 15.0, Epsilon: 0.786\n",
            "Episode 49, Reward: 61.0, Epsilon: 0.782\n",
            "Episode 50, Reward: 67.0, Epsilon: 0.778\n",
            "Episode 51, Reward: 30.0, Epsilon: 0.774\n",
            "Episode 52, Reward: 10.0, Epsilon: 0.771\n",
            "Episode 53, Reward: 63.0, Epsilon: 0.767\n",
            "Episode 54, Reward: 35.0, Epsilon: 0.763\n",
            "Episode 55, Reward: 63.0, Epsilon: 0.759\n",
            "Episode 56, Reward: 72.0, Epsilon: 0.755\n",
            "Episode 57, Reward: 23.0, Epsilon: 0.751\n",
            "Episode 58, Reward: 33.0, Epsilon: 0.748\n",
            "Episode 59, Reward: 32.0, Epsilon: 0.744\n",
            "Episode 60, Reward: 13.0, Epsilon: 0.740\n",
            "Episode 61, Reward: 49.0, Epsilon: 0.737\n",
            "Episode 62, Reward: 11.0, Epsilon: 0.733\n",
            "Episode 63, Reward: 13.0, Epsilon: 0.729\n",
            "Episode 64, Reward: 43.0, Epsilon: 0.726\n",
            "Episode 65, Reward: 49.0, Epsilon: 0.722\n",
            "Episode 66, Reward: 49.0, Epsilon: 0.718\n",
            "Episode 67, Reward: 13.0, Epsilon: 0.715\n",
            "Episode 68, Reward: 28.0, Epsilon: 0.711\n",
            "Episode 69, Reward: 38.0, Epsilon: 0.708\n",
            "Episode 70, Reward: 17.0, Epsilon: 0.704\n",
            "Episode 71, Reward: 18.0, Epsilon: 0.701\n",
            "Episode 72, Reward: 21.0, Epsilon: 0.697\n",
            "Episode 73, Reward: 33.0, Epsilon: 0.694\n",
            "Episode 74, Reward: 46.0, Epsilon: 0.690\n",
            "Episode 75, Reward: 32.0, Epsilon: 0.687\n",
            "Episode 76, Reward: 38.0, Epsilon: 0.683\n",
            "Episode 77, Reward: 35.0, Epsilon: 0.680\n",
            "Episode 78, Reward: 36.0, Epsilon: 0.676\n",
            "Episode 79, Reward: 50.0, Epsilon: 0.673\n",
            "Episode 80, Reward: 31.0, Epsilon: 0.670\n",
            "Episode 81, Reward: 19.0, Epsilon: 0.666\n",
            "Episode 82, Reward: 20.0, Epsilon: 0.663\n",
            "Episode 83, Reward: 165.0, Epsilon: 0.660\n",
            "Episode 84, Reward: 42.0, Epsilon: 0.656\n",
            "Episode 85, Reward: 17.0, Epsilon: 0.653\n",
            "Episode 86, Reward: 26.0, Epsilon: 0.650\n",
            "Episode 87, Reward: 157.0, Epsilon: 0.647\n",
            "Episode 88, Reward: 23.0, Epsilon: 0.643\n",
            "Episode 89, Reward: 139.0, Epsilon: 0.640\n",
            "Episode 90, Reward: 65.0, Epsilon: 0.637\n",
            "Episode 91, Reward: 24.0, Epsilon: 0.634\n",
            "Episode 92, Reward: 12.0, Epsilon: 0.631\n",
            "Episode 93, Reward: 96.0, Epsilon: 0.627\n",
            "Episode 94, Reward: 47.0, Epsilon: 0.624\n",
            "Episode 95, Reward: 41.0, Epsilon: 0.621\n",
            "Episode 96, Reward: 59.0, Epsilon: 0.618\n",
            "Episode 97, Reward: 103.0, Epsilon: 0.615\n",
            "Episode 98, Reward: 32.0, Epsilon: 0.612\n",
            "Episode 99, Reward: 17.0, Epsilon: 0.609\n",
            "Episode 100, Reward: 56.0, Epsilon: 0.606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "watch_trained_agent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "LTn9GFxcoQB0",
        "outputId": "3dec2614-e87c-4da8-de09-233047307b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'watch_trained_agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7e8f52ff5593>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwatch_trained_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'watch_trained_agent' is not defined"
          ]
        }
      ]
    }
  ]
}