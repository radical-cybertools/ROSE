{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb16eae7-efd2-47df-9211-30d22523c8c8",
   "metadata": {},
   "source": [
    "# What is ROSE and why?\n",
    "\n",
    "**ROSE**: RADICAL Orchestrator for Surrogate Exploration (ROSE) toolkit is a framework designed to enable the concurrent and adaptive execution of simulation, surrogate training, and selection tasks on High-Performance Computing (HPC) resources. ROSE allows you to enable, scale, and accelerate your learning workflows across thousands of CPU cores and GPUs effectively and efficiently with just a few lines of code \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"attachment:dfdde6a0-c257-4aa7-bca8-8817ec693214.png\" \n",
    "       width=\"800\" \n",
    "       style=\"border: 3px solid black; border-radius: 10px; margin: 5px;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "* A framework that facilitates building and executing ML learning workflows on different HPC clusters seamlessly.\n",
    "* Offers multiple standardized AL, RL, UQ, and HPO (upcoming) approaches, while also allowing the flexibility to create custom ML workflows.\n",
    "* Built on RCT, leveraging its runtime capabilities to execute heterogeneous tasks on heterogeneous resources.\n",
    "* Open-source and Python-native.\n",
    "\n",
    "\n",
    "### Why ROSE\n",
    "\n",
    "Today’s challenge: Scientists must wire together training, simulation, and learning tasks — this is manual, fragile, not scalable, or performance efficient.\n",
    "\n",
    "ROSE advantage:\n",
    "* Automates, scales, portabilizes, and asynchronously executes workflow learners (training, active learning, UQ, RL) on a large scale on HPC efficently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db84413d-93e9-4715-b4b8-f4d7abada57b",
   "metadata": {},
   "source": [
    "# Standardized Approach for building Learners\n",
    "\n",
    "* Learner is a `unified` workflow that can contain `simulation` --> `training` --> `active learn` steps (tasks) and can be automated.\n",
    "* Learners are often isolated and run asynchronously. ROSE also extends this notion to support learners who can communicate and depend on others accordingly.\n",
    "* Within each learner user can parallelize their `simulation` --> `training` --> `active learn` steps in the best way that fits their resources and needs.\n",
    "* Additionally, users can benefit from ROSE by leveraging learner-level parallelization, allowing them to submit N parallel learners and letting ROSE manage that for them on HPC.\n",
    "\n",
    "## Outlines:\n",
    "This tutorial will teach the user into how to use ROSE to build:\n",
    "* Basic Surrogate Learner.\n",
    "* Parallel Surrogate Learners.\n",
    "* Ensemble Surrogate Learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b1a731-e17c-4db0-a0b6-f7d8a3d09614",
   "metadata": {},
   "source": [
    "## Sequential Learner\n",
    "\n",
    "In this example, we will learn how to use ROSE API to build and submit a `single` Active Learn Learner that either stops when the performance metric threshold is `met` or the number of iterations the user specified is reached (in this case, 5 iterations).\n",
    "\n",
    "```\n",
    "                                                  ┌────────────────────────┐\n",
    "                                                  │      SIMULATION        │\n",
    "                                                  │ (Generate synthetic    │\n",
    "                                                  │  or real-world data)   │\n",
    "                                                  └────────────┬───────────┘\n",
    "                                                               │\n",
    "                                                               ▼\n",
    "                                                  ┌────────────────────────┐\n",
    "                                                  │        TRAINING        │\n",
    "                                                  │ (Train model on        │\n",
    "                                                  │  initial dataset)      │\n",
    "                                                  └────────────┬───────────┘\n",
    "                                                               │\n",
    "                                                               ▼\n",
    "                                                  ┌────────────────────────┐\n",
    "                                                  │     ACTIVE LEARNING    │\n",
    "                                                  │ (Use model to query or │\n",
    "                                                  │  label uncertain data) │\n",
    "                                                  └────────────┬───────────┘\n",
    "                                                               │\n",
    "                                                               ▼\n",
    "                                                  ┌────────────────────────┐\n",
    "                                                  │   IMPROVED MODEL LOOP  │\n",
    "                                                  │(Repeat for N iters     │\n",
    "                                                  │   or performance goal) │\n",
    "                                                  └────────────────────────┘\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ae0aae-91a7-4be9-8ff2-9ffab7b64a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import asyncio\n",
    "import logging\n",
    "\n",
    "from typing import Dict, List, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# learner and task level imports\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ROSE top layer imports\n",
    "from rose.metrics import MEAN_SQUARED_ERROR_MSE\n",
    "from rose.al.active_learner import Learner, SequentialActiveLearner, ParallelActiveLearner\n",
    "\n",
    "# ROSE Bottom layers imports\n",
    "from radical.asyncflow import WorkflowEngine\n",
    "from radical.asyncflow import ConcurrentExecutionBackend\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from radical.asyncflow.logging import init_default_logger\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "812ebf93-35a7-4915-a632-b314faeeecbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m2025-10-26 19:42:34.222\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[root]\u001b[0m │ Logger configured successfully - Console: INFO, File: disabled (N/A), Structured: disabled, Style: modern\n",
      "\u001b[90m2025-10-26 19:42:34.223\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Starting Active Learning with ROSE...\n",
      "Starting Active Learner\n",
      "Starting Iteration-0\n",
      "\u001b[90m2025-10-26 19:42:34.224\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation'] for execution\n",
      "\u001b[90m2025-10-26 19:42:34.299\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating data ===\n",
      "\u001b[90m2025-10-26 19:42:34.302\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Initial labeled: 10, Pool: 50\n",
      "\u001b[90m2025-10-26 19:42:34.312\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000001 is in DONE state\n",
      "\u001b[90m2025-10-26 19:42:35.308\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training'] for execution\n",
      "\u001b[90m2025-10-26 19:42:35.321\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 0 ===\n",
      "\u001b[90m2025-10-26 19:42:35.350\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 10, Test RMSE: 0.2109\n",
      "\u001b[90m2025-10-26 19:42:35.363\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000002 is in DONE state\n",
      "\u001b[90m2025-10-26 19:42:36.335\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['active_learn'] for execution\n",
      "\u001b[90m2025-10-26 19:42:36.346\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting samples (Iteration 0) ===\n",
      "\u001b[90m2025-10-26 19:42:36.352\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples\n",
      "\u001b[90m2025-10-26 19:42:36.355\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Labeled: 10 -> 15\n",
      "\u001b[90m2025-10-26 19:42:36.368\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000003 is in DONE state\n",
      "\u001b[90m2025-10-26 19:42:37.360\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation'] for execution\n",
      "\u001b[90m2025-10-26 19:42:37.371\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating data ===\n",
      "\u001b[90m2025-10-26 19:42:37.374\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Initial labeled: 10, Pool: 50\n",
      "\u001b[90m2025-10-26 19:42:37.381\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000004 is in DONE state\n",
      "\u001b[90m2025-10-26 19:42:38.385\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training'] for execution\n",
      "\u001b[90m2025-10-26 19:42:38.397\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 0 ===\n",
      "\u001b[90m2025-10-26 19:42:38.429\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 10, Test RMSE: 0.2109\n",
      "\u001b[90m2025-10-26 19:42:38.440\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000005 is in DONE state\n",
      "Starting Iteration-1\n",
      "\u001b[90m2025-10-26 19:42:38.442\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['active_learn'] for execution\n",
      "\u001b[90m2025-10-26 19:42:38.456\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting samples (Iteration 0) ===\n",
      "\u001b[90m2025-10-26 19:42:38.467\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples\n",
      "\u001b[90m2025-10-26 19:42:38.471\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Labeled: 10 -> 15\n",
      "\u001b[90m2025-10-26 19:42:38.489\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000006 is in DONE state\n",
      "\u001b[90m2025-10-26 19:42:39.470\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation'] for execution\n",
      "\u001b[90m2025-10-26 19:42:39.481\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating data ===\n",
      "\u001b[90m2025-10-26 19:42:39.485\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Initial labeled: 10, Pool: 50\n",
      "\u001b[90m2025-10-26 19:42:39.491\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000007 is in DONE state\n",
      "\u001b[90m2025-10-26 19:42:40.495\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training'] for execution\n",
      "\u001b[90m2025-10-26 19:42:40.505\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 0 ===\n",
      "\u001b[90m2025-10-26 19:42:40.534\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 10, Test RMSE: 0.2109\n",
      "\u001b[90m2025-10-26 19:42:40.542\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000008 is in DONE state\n",
      "Starting Iteration-2\n",
      "\u001b[90m2025-10-26 19:42:40.544\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['active_learn'] for execution\n",
      "\u001b[90m2025-10-26 19:42:40.552\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting samples (Iteration 0) ===\n",
      "\u001b[90m2025-10-26 19:42:40.560\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples\n",
      "\u001b[90m2025-10-26 19:42:40.565\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Labeled: 10 -> 15\n",
      "\u001b[90m2025-10-26 19:42:40.578\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000009 is in DONE state\n",
      "\u001b[90m2025-10-26 19:42:41.574\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation'] for execution\n",
      "\u001b[90m2025-10-26 19:42:41.581\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating data ===\n",
      "\u001b[90m2025-10-26 19:42:41.586\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Initial labeled: 10, Pool: 50\n",
      "\u001b[90m2025-10-26 19:42:41.597\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000010 is in DONE state\n",
      "\u001b[90m2025-10-26 19:42:42.599\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training'] for execution\n",
      "\u001b[90m2025-10-26 19:42:42.606\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 0 ===\n",
      "\u001b[90m2025-10-26 19:42:42.637\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 10, Test RMSE: 0.2109\n",
      "\u001b[90m2025-10-26 19:42:42.644\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000011 is in DONE state\n",
      "Starting Iteration-3\n",
      "\u001b[90m2025-10-26 19:42:42.646\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['active_learn'] for execution\n",
      "\u001b[90m2025-10-26 19:42:42.655\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting samples (Iteration 0) ===\n",
      "\u001b[90m2025-10-26 19:42:42.665\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples\n",
      "\u001b[90m2025-10-26 19:42:42.672\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Labeled: 10 -> 15\n",
      "\u001b[90m2025-10-26 19:42:42.686\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000012 is in DONE state\n",
      "\u001b[90m2025-10-26 19:42:43.674\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation'] for execution\n",
      "\u001b[90m2025-10-26 19:42:43.682\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating data ===\n",
      "\u001b[90m2025-10-26 19:42:43.686\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Initial labeled: 10, Pool: 50\n",
      "\u001b[90m2025-10-26 19:42:43.698\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000013 is in DONE state\n",
      "\u001b[90m2025-10-26 19:42:44.700\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training'] for execution\n",
      "\u001b[90m2025-10-26 19:42:44.707\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 0 ===\n",
      "\u001b[90m2025-10-26 19:42:44.762\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 10, Test RMSE: 0.2109\n",
      "\u001b[90m2025-10-26 19:42:44.768\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000014 is in DONE state\n",
      "Starting Iteration-4\n",
      "\u001b[90m2025-10-26 19:42:44.770\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['active_learn'] for execution\n",
      "\u001b[90m2025-10-26 19:42:44.777\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting samples (Iteration 0) ===\n",
      "\u001b[90m2025-10-26 19:42:44.788\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples\n",
      "\u001b[90m2025-10-26 19:42:44.792\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Labeled: 10 -> 15\n",
      "\u001b[90m2025-10-26 19:42:44.937\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000015 is in DONE state\n",
      "\u001b[90m2025-10-26 19:42:45.796\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation'] for execution\n",
      "\u001b[90m2025-10-26 19:42:45.803\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating data ===\n",
      "\u001b[90m2025-10-26 19:42:45.809\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Initial labeled: 10, Pool: 50\n",
      "\u001b[90m2025-10-26 19:42:45.821\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000016 is in DONE state\n",
      "\u001b[90m2025-10-26 19:42:46.822\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training'] for execution\n",
      "\u001b[90m2025-10-26 19:42:46.828\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 0 ===\n",
      "\u001b[90m2025-10-26 19:42:46.859\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 10, Test RMSE: 0.2109\n",
      "\u001b[90m2025-10-26 19:42:46.865\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000017 is in DONE state\n",
      "\u001b[90m2025-10-26 19:42:46.866\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Active Learning completed!\n",
      "\u001b[90m2025-10-26 19:42:46.867\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Initiating shutdown\n",
      "\u001b[90m2025-10-26 19:42:46.909\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[execution.backend(concurrent)]\u001b[0m │ Concurrent execution backend shutdown complete\n",
      "\u001b[90m2025-10-26 19:42:46.909\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Shutdown completed for all components.\n"
     ]
    }
   ],
   "source": [
    "async def run(acl, **kwargs):\n",
    "    INITIAL_SAMPLES = 10\n",
    "    SAMPLES_PER_ITER = 5\n",
    "    POOL_SIZE = 50\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 1. SIMULATION TASK\n",
    "    # ========================================================================\n",
    "    @acl.simulation_task(as_executable=False)\n",
    "    async def simulation(*args, **kwargs) -> dict:\n",
    "        logger.info(\"=== SIMULATION: Generating data ===\")\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        X_all = np.linspace(0, 4 * np.pi, INITIAL_SAMPLES + POOL_SIZE).reshape(-1, 1)\n",
    "        y_all = np.sin(X_all).ravel() + np.random.normal(0, 0.1, X_all.shape[0])\n",
    "        \n",
    "        indices = np.random.permutation(len(X_all))\n",
    "        labeled_idx = indices[:INITIAL_SAMPLES]\n",
    "        pool_idx = indices[INITIAL_SAMPLES:]\n",
    "        \n",
    "        X_test = np.linspace(0, 4 * np.pi, 100).reshape(-1, 1)\n",
    "        y_test = np.sin(X_test).ravel()\n",
    "        \n",
    "        logger.info(f\"Initial labeled: {len(labeled_idx)}, Pool: {len(pool_idx)}\")\n",
    "        \n",
    "        return {\n",
    "            \"X_all\": X_all,\n",
    "            \"y_all\": y_all,\n",
    "            \"labeled_indices\": labeled_idx.tolist(),\n",
    "            \"pool_indices\": pool_idx.tolist(),\n",
    "            \"X_test\": X_test,\n",
    "            \"y_test\": y_test\n",
    "        }\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 2. TRAINING TASK\n",
    "    # ========================================================================\n",
    "    @acl.training_task(as_executable=False)\n",
    "    async def training(*args, **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Training depends on: simulation_task\n",
    "        Args:\n",
    "            args[0]: simulation result (data dict)\n",
    "        \"\"\"\n",
    "        # Get data from args[0] (simulation result)\n",
    "        data = args[0] if args else kwargs.get(\"data\", {})\n",
    "        iteration = kwargs.get(\"iteration\", 0)\n",
    "        \n",
    "        logger.info(f\"=== TRAINING: Iteration {iteration} ===\")\n",
    "        \n",
    "        X_all = data[\"X_all\"]\n",
    "        y_all = data[\"y_all\"]\n",
    "        labeled_idx = np.array(data[\"labeled_indices\"])\n",
    "        X_test = data[\"X_test\"]\n",
    "        y_test = data[\"y_test\"]\n",
    "        \n",
    "        X_train = X_all[labeled_idx]\n",
    "        y_train = y_all[labeled_idx]\n",
    "        \n",
    "        kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=0.1)\n",
    "        model = GaussianProcessRegressor(kernel=kernel, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred, y_std = model.predict(X_test, return_std=True)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        logger.info(f\"Training samples: {len(X_train)}, Test RMSE: {rmse:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            \"model\": model,\n",
    "            \"rmse\": rmse,\n",
    "            \"mean_uncertainty\": np.mean(y_std),\n",
    "            \"labeled_indices\": labeled_idx.tolist(),\n",
    "            \"iteration\": iteration\n",
    "        }\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 3. ACTIVE LEARNING TASK\n",
    "    # ========================================================================\n",
    "    @acl.active_learn_task(as_executable=False)\n",
    "    async def active_learn(*args, **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Active learning depends on: (simulation_task, training_task)\n",
    "        Args:\n",
    "            args[0]: simulation result (data dict)\n",
    "            args[1]: training result (model + metrics dict)\n",
    "        \"\"\"\n",
    "        simulation_result = args[0]  # From sim_task\n",
    "        training_result = args[1]    # From train_task\n",
    "\n",
    "        iteration = training_result.get(\"iteration\", 0)\n",
    "        \n",
    "        logger.info(f\"=== ACTIVE LEARNING: Selecting samples (Iteration {iteration}) ===\")\n",
    "        \n",
    "        # Now extract data\n",
    "        model = training_result[\"model\"]\n",
    "        X_all = simulation_result[\"X_all\"]\n",
    "        labeled_idx = np.array(training_result[\"labeled_indices\"])\n",
    "        pool_idx = np.array(simulation_result[\"pool_indices\"])\n",
    "        \n",
    "        # Predict uncertainty on pool\n",
    "        X_pool = X_all[pool_idx]\n",
    "        _, y_std = model.predict(X_pool, return_std=True)\n",
    "        \n",
    "        # Select top-k most uncertain\n",
    "        uncertain_indices = np.argsort(y_std)[-SAMPLES_PER_ITER:]\n",
    "        selected_pool_idx = pool_idx[uncertain_indices]\n",
    "        \n",
    "        # Update indices\n",
    "        new_labeled = np.concatenate([labeled_idx, selected_pool_idx])\n",
    "        new_pool = np.setdiff1d(pool_idx, selected_pool_idx)\n",
    "        \n",
    "        logger.info(f\"Selected {len(selected_pool_idx)} uncertain samples\")\n",
    "        logger.info(f\"Labeled: {len(labeled_idx)} -> {len(new_labeled)}\")\n",
    "        \n",
    "        # Return updated data for next iteration\n",
    "        return {\n",
    "            \"X_all\": simulation_result[\"X_all\"],\n",
    "            \"y_all\": simulation_result[\"y_all\"],\n",
    "            \"labeled_indices\": new_labeled.tolist(),\n",
    "            \"pool_indices\": new_pool.tolist(),\n",
    "            \"X_test\": simulation_result[\"X_test\"],\n",
    "            \"y_test\": simulation_result[\"y_test\"],\n",
    "            \"iteration\": iteration + 1\n",
    "        }\n",
    "\n",
    "    # Run\n",
    "    logger.info(\"Starting Active Learning with ROSE...\")\n",
    "    await acl.teach(**kwargs)\n",
    "    logger.info(\"Active Learning completed!\")\n",
    "\n",
    "try:\n",
    "    engine = await ConcurrentExecutionBackend(ProcessPoolExecutor())\n",
    "    asyncflow = await WorkflowEngine.create(engine)\n",
    "    acl = SequentialActiveLearner(asyncflow)\n",
    "\n",
    "    init_default_logger(logging.INFO)\n",
    "    await run(acl, max_iter=5)\n",
    "except Exception as e:\n",
    "    print(f'Learner Failed with: {e}')\n",
    "finally:\n",
    "    await acl.shutdown()\n",
    "    logging.getLogger().handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73637a0f-aade-4d8f-b9b7-c0f4ab4ee4f4",
   "metadata": {},
   "source": [
    "#### But what if I want to stop the execution earlier if my performance metric is reached?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df830e3c-9362-47eb-bb38-e1cabaa9beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@acl.as_stop_criterion(metric_name=MEAN_SQUARED_ERROR_MSE, threshold=0.1, as_executable=False)\n",
    "async def check_mse():\n",
    "    \"\"\"\n",
    "    Your stop condition\n",
    "    1-For each AL iteration, check if MSE <= 0.1\n",
    "    2-Terminate early if reached\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d1ce5-50b1-47a4-8697-d4e7812ffae5",
   "metadata": {},
   "source": [
    "## Parallel Learners\n",
    "\n",
    "When we talk about parallel learners, we mean running multiple surrogates at the same time, instead of just one. Different learners can explore different parts of the data space. Running in parallel takes advantage of modern multi-core CPUs or GPUs.\n",
    "\n",
    "```\n",
    "                   SHARED SIMULATION                  |    OWN SIMULATION PER LEARNER\n",
    "                   ───────────────────────────────────|────────────────────────────────────────\n",
    "                           ┌───────────────┐          |        ┌───────────────┐\n",
    "                           │ SIMULATION    │          |        │ SIMULATION 1  │\n",
    "                           │ (Shared)      │          |        │ (Learner 1)   │\n",
    "                           └──────┬────────┘          |        └──────┬────────┘\n",
    "                                  │                   |               │\n",
    "                    ┌─────────────┴─────────────┐     | ┌─────────────┴─────────────┐\n",
    "                    │       TRAINING 1          │     | │          TRAINING 1       │\n",
    "                    │       TRAINING 2          │     | │          TRAINING 2       │\n",
    "                    │       TRAINING N          │     | │          TRAINING N       │\n",
    "                    └─────────────┬─────────────┘     | └─────────────┬─────────────┘\n",
    "                                  │                   |               │\n",
    "                    ┌─────────────┴─────────────┐     | ┌─────────────┴─────────────┐\n",
    "                    │   ACTIVE LEARNING 1..N    │     | │     ACTIVE LEARNING 1     │\n",
    "                    └─────────────┬─────────────┘     | │     ACTIVE LEARNING 2     │\n",
    "                                  │                   | │     ACTIVE LEARNING N     │\n",
    "                    ┌─────────────┴─────────────┐     | └─────────────┬─────────────┘\n",
    "                    │   IMPROVED LOOP 1..N      │     | ┌─────────────┴─────────────┐\n",
    "                    └───────────────────────────┘     | │     IMPROVED LOOP 1       │\n",
    "                                                      | │     IMPROVED LOOP 2       │\n",
    "                                                      | │     IMPROVED LOOP N       │\n",
    "                                                      | └───────────────────────────┘\n",
    "                               \n",
    "\n",
    "```\n",
    "\n",
    "In ROSE, scaling your Learners on HPC is done by using the `Parallel Learners` instead of the `Sequential Learners`. \n",
    "\n",
    "The example below will show how to build and launch 5 truly asynchronous and parallel learners on your compute resources in a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51bf4738-8472-4500-be2c-fc798d746891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m2025-10-26 19:43:06.040\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[root]\u001b[0m │ Logger configured successfully - Console: INFO, File: disabled (N/A), Structured: disabled, Style: modern\n",
      "\u001b[90m2025-10-26 19:43:06.041\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Starting Active Learning with ROSE...\n",
      "Starting Parallel Active Learning with 2 learners\n",
      "Starting Active Learner (Learner-0)\n",
      "[Learner-0] Starting Iteration-0\n",
      "Starting Active Learner (Learner-1)\n",
      "[Learner-1] Starting Iteration-0\n",
      "\u001b[90m2025-10-26 19:43:06.043\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation', 'simulation'] for execution\n",
      "\u001b[90m2025-10-26 19:43:06.114\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating data ===\n",
      "\u001b[90m2025-10-26 19:43:06.114\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating data ===\n",
      "\u001b[90m2025-10-26 19:43:06.117\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Initial labeled: 10, Pool: 50\n",
      "\u001b[90m2025-10-26 19:43:06.117\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Initial labeled: 10, Pool: 50\n",
      "\u001b[90m2025-10-26 19:43:06.125\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000001 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:06.128\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000006 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:07.123\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training', 'training'] for execution\n",
      "\u001b[90m2025-10-26 19:43:07.133\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 0 ===\n",
      "\u001b[90m2025-10-26 19:43:07.133\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 0 ===\n",
      "\u001b[90m2025-10-26 19:43:07.162\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 10, Test RMSE: 0.2109\n",
      "\u001b[90m2025-10-26 19:43:07.162\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 10, Test RMSE: 0.2109\n",
      "\u001b[90m2025-10-26 19:43:07.181\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000002 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:07.185\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000007 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:08.150\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['active_learn', 'active_learn'] for execution\n",
      "\u001b[90m2025-10-26 19:43:08.167\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting samples (Iteration 0) ===\n",
      "\u001b[90m2025-10-26 19:43:08.167\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting samples (Iteration 0) ===\n",
      "\u001b[90m2025-10-26 19:43:08.175\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples\n",
      "\u001b[90m2025-10-26 19:43:08.173\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples\n",
      "\u001b[90m2025-10-26 19:43:08.179\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Labeled: 10 -> 15\n",
      "\u001b[90m2025-10-26 19:43:08.186\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Labeled: 10 -> 15\n",
      "\u001b[90m2025-10-26 19:43:08.202\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000003 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:08.213\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000008 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:09.177\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation', 'simulation'] for execution\n",
      "\u001b[90m2025-10-26 19:43:09.192\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating data ===\n",
      "\u001b[90m2025-10-26 19:43:09.192\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating data ===\n",
      "\u001b[90m2025-10-26 19:43:09.195\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Initial labeled: 10, Pool: 50\n",
      "\u001b[90m2025-10-26 19:43:09.195\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Initial labeled: 10, Pool: 50\n",
      "\u001b[90m2025-10-26 19:43:09.201\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000004 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:09.204\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000009 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:10.203\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training', 'training'] for execution\n",
      "\u001b[90m2025-10-26 19:43:10.211\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 0 ===\n",
      "\u001b[90m2025-10-26 19:43:10.211\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 0 ===\n",
      "\u001b[90m2025-10-26 19:43:10.252\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 10, Test RMSE: 0.2109\n",
      "\u001b[90m2025-10-26 19:43:10.252\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 10, Test RMSE: 0.2109\n",
      "\u001b[90m2025-10-26 19:43:10.261\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000010 is in DONE state\n",
      "[Learner-1] Starting Iteration-1\n",
      "\u001b[90m2025-10-26 19:43:10.265\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['active_learn'] for execution\n",
      "\u001b[90m2025-10-26 19:43:10.272\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000005 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:10.278\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting samples (Iteration 0) ===\n",
      "[Learner-0] Starting Iteration-1\n",
      "\u001b[90m2025-10-26 19:43:10.280\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['active_learn'] for execution\n",
      "\u001b[90m2025-10-26 19:43:10.283\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples\n",
      "\u001b[90m2025-10-26 19:43:10.288\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting samples (Iteration 0) ===\n",
      "\u001b[90m2025-10-26 19:43:10.288\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Labeled: 10 -> 15\n",
      "\u001b[90m2025-10-26 19:43:10.296\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples\n",
      "\u001b[90m2025-10-26 19:43:10.304\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Labeled: 10 -> 15\n",
      "\u001b[90m2025-10-26 19:43:10.314\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000011 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:10.346\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000014 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:10.356\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation'] for execution\n",
      "\u001b[90m2025-10-26 19:43:10.365\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating data ===\n",
      "\u001b[90m2025-10-26 19:43:10.371\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation'] for execution\n",
      "\u001b[90m2025-10-26 19:43:10.369\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Initial labeled: 10, Pool: 50\n",
      "\u001b[90m2025-10-26 19:43:10.376\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating data ===\n",
      "\u001b[90m2025-10-26 19:43:10.382\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Initial labeled: 10, Pool: 50\n",
      "\u001b[90m2025-10-26 19:43:10.382\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000012 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:10.385\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000015 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:10.397\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training', 'training'] for execution\n",
      "\u001b[90m2025-10-26 19:43:10.400\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 0 ===\n",
      "\u001b[90m2025-10-26 19:43:10.400\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 0 ===\n",
      "\u001b[90m2025-10-26 19:43:10.423\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 10, Test RMSE: 0.2109\n",
      "\u001b[90m2025-10-26 19:43:10.423\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 10, Test RMSE: 0.2109\n",
      "\u001b[90m2025-10-26 19:43:10.429\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000013 is in DONE state\n",
      "[Learner-1] Starting Iteration-2\n",
      "\u001b[90m2025-10-26 19:43:10.434\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000016 is in DONE state\n",
      "[Learner-0] Starting Iteration-2\n",
      "\u001b[90m2025-10-26 19:43:10.437\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['active_learn', 'active_learn'] for execution\n",
      "\u001b[90m2025-10-26 19:43:10.447\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting samples (Iteration 0) ===\n",
      "\u001b[90m2025-10-26 19:43:10.452\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === ACTIVE LEARNING: Selecting samples (Iteration 0) ===\n",
      "\u001b[90m2025-10-26 19:43:10.457\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples\n",
      "\u001b[90m2025-10-26 19:43:10.461\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Labeled: 10 -> 15\n",
      "\u001b[90m2025-10-26 19:43:10.465\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Selected 5 uncertain samples\n",
      "\u001b[90m2025-10-26 19:43:10.488\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000017 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:10.474\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Labeled: 10 -> 15\n",
      "\u001b[90m2025-10-26 19:43:10.515\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000020 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:11.467\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['simulation', 'simulation'] for execution\n",
      "\u001b[90m2025-10-26 19:43:11.475\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating data ===\n",
      "\u001b[90m2025-10-26 19:43:11.476\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === SIMULATION: Generating data ===\n",
      "\u001b[90m2025-10-26 19:43:11.477\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Initial labeled: 10, Pool: 50\n",
      "\u001b[90m2025-10-26 19:43:11.477\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Initial labeled: 10, Pool: 50\n",
      "\u001b[90m2025-10-26 19:43:11.483\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000018 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:11.487\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000021 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:11.495\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training'] for execution\n",
      "\u001b[90m2025-10-26 19:43:11.498\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 0 ===\n",
      "\u001b[90m2025-10-26 19:43:11.507\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['training'] for execution\n",
      "\u001b[90m2025-10-26 19:43:11.510\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ === TRAINING: Iteration 0 ===\n",
      "\u001b[90m2025-10-26 19:43:11.526\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 10, Test RMSE: 0.2109\n",
      "\u001b[90m2025-10-26 19:43:11.532\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000019 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:11.542\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training samples: 10, Test RMSE: 0.2109\n",
      "\u001b[90m2025-10-26 19:43:11.547\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000022 is in DONE state\n",
      "\u001b[90m2025-10-26 19:43:11.548\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Active Learning completed!\n",
      "\u001b[90m2025-10-26 19:43:11.548\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Initiating shutdown\n",
      "\u001b[90m2025-10-26 19:43:11.562\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[execution.backend(concurrent)]\u001b[0m │ Concurrent execution backend shutdown complete\n",
      "\u001b[90m2025-10-26 19:43:11.563\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Shutdown completed for all components.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    engine = await ConcurrentExecutionBackend(ProcessPoolExecutor())\n",
    "    asyncflow = await WorkflowEngine.create(engine)\n",
    "    acl = ParallelActiveLearner(asyncflow)\n",
    "\n",
    "    init_default_logger(logging.INFO)\n",
    "    await run(acl, max_iter=3, parallel_learners=2)\n",
    "except Exception as e:\n",
    "    print(f'Learner Failed with: {e}')\n",
    "finally:\n",
    "    await acl.shutdown()\n",
    "    logging.getLogger().handlers.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255e4bc-fb2a-472a-8761-8a1d6bf4f964",
   "metadata": {},
   "source": [
    "### Building Ensembe Surrogates with ROSE\n",
    "When we talk about ensemble surrogates, we mean training multiple models at the same time, each with different hyperparameters, instead of one after another. Different models explore different parts of the parameter space, giving us better predictions and uncertainty estimates. In ROSE, scaling your ensemble training to 100+ models is done by submitting in parallel, leveraging the higher-level parallelism of learners and tasks. \n",
    "\n",
    "```\n",
    "ENSEMBLE SURROGATE WORKFLOW (ROSE)\n",
    "===================================\n",
    "\n",
    "                    ┌─────────────────┐\n",
    "                    │  Training Data  │\n",
    "                    │  (X, y)         │\n",
    "                    └────────┬────────┘\n",
    "                             │\n",
    "                             ▼\n",
    "              ┌──────────────────────────────┐\n",
    "              │  Generate N Configurations   │\n",
    "              │  (diverse hyperparameters)   │\n",
    "              └──────────────┬───────────────┘\n",
    "                             │\n",
    "        ┌────────────────────┼────────────────────┐\n",
    "        │                    │                    │\n",
    "        ▼                    ▼                    ▼\n",
    "   ┌─────────┐          ┌─────────┐          ┌─────────┐\n",
    "   │ Config1 │          │ Config2 │   ...    │ ConfigN │\n",
    "   │ ls=0.5  │          │ ls=1.2  │          │ ls=1.8  │\n",
    "   └────┬────┘          └────┬────┘          └────┬────┘\n",
    "        │                    │                    │\n",
    "        ▼                    ▼                    ▼\n",
    "   ╔═════════╗          ╔═════════╗          ╔═════════╗\n",
    "   ║ TRAIN 1 ║          ║ TRAIN 2 ║   ...    ║ TRAIN N ║  ← PARALLEL\n",
    "   ║ (async) ║          ║ (async) ║          ║ (async) ║\n",
    "   ╚════┬════╝          ╚════┬════╝          ╚════┬════╝\n",
    "        │                    │                    │\n",
    "        └────────────────────┼────────────────────┘\n",
    "                             │\n",
    "                             ▼\n",
    "                    ┌─────────────────┐\n",
    "                    │ Ensemble Combine│\n",
    "                    │  • Average pred │\n",
    "                    │  • Uncertainty  │\n",
    "                    └────────┬────────┘\n",
    "                             │\n",
    "                             ▼\n",
    "                    ┌─────────────────┐\n",
    "                    │ Robust Surrogate│\n",
    "                    │ Ready for use!  │\n",
    "                    └─────────────────┘\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "The example below will show how to build and train a robust ensemble of surrogate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e36c7-f785-4cd7-b7e0-25115be17a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m2025-10-26 19:44:10.383\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[root]\u001b[0m │ Logger configured successfully - Console: INFO, File: disabled (N/A), Structured: disabled, Style: modern\n",
      "\u001b[90m2025-10-26 19:44:10.386\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[execution.backend(concurrent)]\u001b[0m │ ProcessPoolExecutor execution backend started successfully\n",
      "\u001b[90m2025-10-26 19:44:10.391\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Generating synthetic stellarator geometry data...\n",
      "\u001b[90m2025-10-26 19:44:10.391\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ ======================================================================\n",
      "\u001b[90m2025-10-26 19:44:10.392\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Training 10 surrogate models in parallel...\n",
      "\u001b[90m2025-10-26 19:44:10.393\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ ======================================================================\n",
      "\u001b[90m2025-10-26 19:44:10.393\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Generated 10 diverse ensemble configurations\n",
      "\u001b[90m2025-10-26 19:44:10.394\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Submitted 10 parallel surrogate training tasks\n",
      "\u001b[90m2025-10-26 19:44:10.396\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train', 'train'] for execution\n",
      "\u001b[90m2025-10-26 19:44:10.484\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 1] Starting training with config: RBF_1\n",
      "\u001b[90m2025-10-26 19:44:10.484\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 0] Starting training with config: RBF_0\n",
      "\u001b[90m2025-10-26 19:44:10.484\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 3] Starting training with config: RBF_3\n",
      "\u001b[90m2025-10-26 19:44:10.484\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 4] Starting training with config: RBF_4\n",
      "\u001b[90m2025-10-26 19:44:10.485\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 5] Starting training with config: RBF_5\n",
      "\u001b[90m2025-10-26 19:44:10.485\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 7] Starting training with config: RBF_7\n",
      "\u001b[90m2025-10-26 19:44:10.484\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 2] Starting training with config: RBF_2\n",
      "\u001b[90m2025-10-26 19:44:10.485\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 6] Starting training with config: RBF_6\n",
      "\u001b[90m2025-10-26 19:44:10.490\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 9] Starting training with config: RBF_9\n",
      "\u001b[90m2025-10-26 19:44:10.488\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 8] Starting training with config: RBF_8\n",
      "\u001b[90m2025-10-26 19:44:13.434\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 8] Completed in 2.94s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-26 19:44:13.457\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000009 is in DONE state\n",
      "\u001b[90m2025-10-26 19:44:13.536\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 2] Completed in 3.04s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-26 19:44:13.560\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000003 is in DONE state\n",
      "\u001b[90m2025-10-26 19:44:13.653\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 5] Completed in 3.16s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-26 19:44:13.671\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000006 is in DONE state\n",
      "\u001b[90m2025-10-26 19:44:13.706\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 9] Completed in 3.21s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-26 19:44:13.724\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000010 is in DONE state\n",
      "\u001b[90m2025-10-26 19:44:13.731\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 6] Completed in 3.23s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-26 19:44:13.745\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000007 is in DONE state\n",
      "\u001b[90m2025-10-26 19:44:13.828\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 4] Completed in 3.33s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-26 19:44:13.842\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000005 is in DONE state\n",
      "\u001b[90m2025-10-26 19:44:13.929\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 7] Completed in 3.44s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-26 19:44:13.942\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000008 is in DONE state\n",
      "\u001b[90m2025-10-26 19:44:14.040\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 3] Completed in 3.55s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-26 19:44:14.051\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000004 is in DONE state\n",
      "\u001b[90m2025-10-26 19:44:14.102\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 1] Completed in 3.61s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-26 19:44:14.108\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ [Model 0] Completed in 3.62s, RMSE: 0.1602\n",
      "\u001b[90m2025-10-26 19:44:14.115\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000002 is in DONE state\n",
      "\u001b[90m2025-10-26 19:44:14.121\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000001 is in DONE state\n",
      "\u001b[90m2025-10-26 19:44:14.124\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Completed: 10/10 models\n",
      "\u001b[90m2025-10-26 19:44:14.127\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Submitting ['ensemble_predict'] for execution\n",
      "\u001b[90m2025-10-26 19:44:14.133\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Combining predictions from 10 models\n",
      "\u001b[90m2025-10-26 19:44:14.138\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ task.000011 is in DONE state\n",
      "\u001b[90m2025-10-26 19:44:14.141\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ ======================================================================\n",
      "\u001b[90m2025-10-26 19:44:14.142\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ ENSEMBLE TRAINING COMPLETE\n",
      "\u001b[90m2025-10-26 19:44:14.144\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ ======================================================================\n",
      "\u001b[90m2025-10-26 19:44:14.146\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Successfully trained: 10 models\n",
      "\u001b[90m2025-10-26 19:44:14.147\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Average time per model: 3.31s\n",
      "\u001b[90m2025-10-26 19:44:14.148\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Total parallel time: 3.62s\n",
      "\u001b[90m2025-10-26 19:44:14.150\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Ensemble Statistics:\n",
      "\u001b[90m2025-10-26 19:44:14.151\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Best single model RMSE: 0.1602\n",
      "\u001b[90m2025-10-26 19:44:14.153\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Ensemble diversity: 0.0000\n",
      "\u001b[90m2025-10-26 19:44:14.154\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[main]\u001b[0m │ Mean uncertainty: 0.1993\n",
      "\u001b[90m2025-10-26 19:44:14.156\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Initiating shutdown\n",
      "\u001b[90m2025-10-26 19:44:14.201\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[execution.backend(concurrent)]\u001b[0m │ Concurrent execution backend shutdown complete\n",
      "\u001b[90m2025-10-26 19:44:14.202\u001b[0m │ \u001b[94mINFO\u001b[0m │ \u001b[38;5;165m[workflow_manager]\u001b[0m │ Shutdown completed for all components.\n"
     ]
    }
   ],
   "source": [
    "init_default_logger(logging.INFO)\n",
    "\n",
    "# Setup backend and workflow engine\n",
    "engine = await ConcurrentExecutionBackend(ProcessPoolExecutor(max_workers=10))\n",
    "asyncflow = await WorkflowEngine.create(engine)\n",
    "learner = Learner(asyncflow)\n",
    "\n",
    "@learner.training_task(as_executable=False)\n",
    "async def train(*args, **kwargs) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Train a single surrogate model\n",
    "    This represents training ONE of the 100+ models in the ensemble\n",
    "    \"\"\"\n",
    "    \n",
    "    model_id = kwargs.get(\"model_id\", 0)\n",
    "    config = kwargs.get(\"config\", {})\n",
    "    train_data = kwargs.get(\"train_data\")\n",
    "    \n",
    "    logger.info(f\"[Model {model_id}] Starting training with config: {config['kernel']}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    X_train = train_data[\"X\"]\n",
    "    y_train = train_data[\"y\"]\n",
    "    X_test = train_data[\"X_test\"]\n",
    "    \n",
    "    # Train surrogate with specific configuration\n",
    "    kernel = RBF(length_scale=config[\"length_scale\"]) + \\\n",
    "             WhiteKernel(noise_level=config[\"noise_level\"])\n",
    "    \n",
    "    model = GaussianProcessRegressor(\n",
    "        kernel=kernel,\n",
    "        n_restarts_optimizer=config.get(\"n_restarts\", 10)\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict with uncertainty\n",
    "    y_pred, y_std = model.predict(X_test, return_std=True)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    y_test = train_data[\"y_test\"]\n",
    "    rmse = np.sqrt(np.mean((y_pred - y_test) ** 2))\n",
    "    \n",
    "    logger.info(f\"[Model {model_id}] Completed in {training_time:.2f}s, RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"model_id\": model_id,\n",
    "        \"model_type\": config[\"model_type\"],\n",
    "        \"kernel_config\": config[\"kernel\"],\n",
    "        \"rmse\": rmse,\n",
    "        \"training_time\": training_time,\n",
    "        \"predictions\": y_pred,\n",
    "        \"uncertainties\": y_std,\n",
    "        \"success\": True\n",
    "    }\n",
    "\n",
    "@learner.utility_task(as_executable=False)\n",
    "async def ensemble_predict(*args, **kwargs) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Combine predictions from an ensemble of surrogates\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    surrogate_results = kwargs.get(\"surrogate_results\", [])\n",
    "    \n",
    "    logger.info(f\"Combining predictions from {len(surrogate_results)} models\")\n",
    "    \n",
    "    # Extract predictions and uncertainties\n",
    "    all_predictions = [r[\"predictions\"] for r in surrogate_results]\n",
    "    all_uncertainties = [r[\"uncertainties\"] for r in surrogate_results]\n",
    "    \n",
    "    # Ensemble prediction strategies\n",
    "    \n",
    "    # 1. Simple averaging\n",
    "    ensemble_mean = np.mean(all_predictions, axis=0)\n",
    "    \n",
    "    # 2. Uncertainty-weighted averaging\n",
    "    weights = 1.0 / (np.array([r[\"rmse\"] for r in surrogate_results]) + 1e-10)\n",
    "    weights = weights / np.sum(weights)\n",
    "    ensemble_weighted = np.average(all_predictions, axis=0, weights=weights)\n",
    "    \n",
    "    # 3. Ensemble uncertainty (variance across models)\n",
    "    ensemble_variance = np.var(all_predictions, axis=0)\n",
    "    ensemble_std = np.sqrt(ensemble_variance)\n",
    "    \n",
    "    # 4. Combined uncertainty (aleatoric + epistemic)\n",
    "    mean_aleatoric = np.mean(all_uncertainties, axis=0)\n",
    "    total_uncertainty = np.sqrt(ensemble_variance + mean_aleatoric**2)\n",
    "    \n",
    "    return {\n",
    "        \"n_models\": len(surrogate_results),\n",
    "        \"ensemble_mean\": ensemble_mean,\n",
    "        \"ensemble_weighted\": ensemble_weighted,\n",
    "        \"ensemble_std\": ensemble_std,\n",
    "        \"total_uncertainty\": total_uncertainty,\n",
    "        \"individual_rmse\": [r[\"rmse\"] for r in surrogate_results],\n",
    "        \"best_model_rmse\": min(r[\"rmse\"] for r in surrogate_results),\n",
    "        \"ensemble_diversity\": float(np.mean(ensemble_variance))\n",
    "    }\n",
    "\n",
    "# run locally so we will not decorate it\n",
    "def generate(n_models: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate diverse configurations for ensemble models\n",
    "    \n",
    "    Key idea: Diversity in the ensemble for better uncertainty quantification\n",
    "    \"\"\"\n",
    "    configs = []\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        config = {\n",
    "            \"model_id\": i,\n",
    "            \"model_type\": \"gpr\",\n",
    "            \"kernel\": f\"RBF_{i}\",\n",
    "            \"length_scale\": np.random.uniform(0.1, 2.0),\n",
    "            \"noise_level\": np.random.uniform(0.001, 0.1),\n",
    "            \"n_restarts\": 10\n",
    "        }\n",
    "        configs.append(config)\n",
    "    \n",
    "    logger.info(f\"Generated {len(configs)} diverse ensemble configurations\")\n",
    "    return configs\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Train an ensemble of N surrogate models in parallel using ROSE\n",
    "    \"\"\"\n",
    "    # Generate synthetic training data (replace with stellarator data)\n",
    "    logger.info(\"Generating synthetic stellarator geometry data...\")\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    n_train = 100\n",
    "    n_test = 20\n",
    "    n_features = 5\n",
    "    \n",
    "    X_train = np.random.randn(n_train, n_features)\n",
    "    y_train = np.sin(X_train[:, 0]) + 0.5 * np.cos(X_train[:, 1]) + \\\n",
    "              np.random.randn(n_train) * 0.1\n",
    "    \n",
    "    X_test = np.random.randn(n_test, n_features)\n",
    "    y_test = np.sin(X_test[:, 0]) + 0.5 * np.cos(X_test[:, 1]) + \\\n",
    "             np.random.randn(n_test) * 0.1\n",
    "\n",
    "    train_data = {\n",
    "        \"X\": X_train,\n",
    "        \"y\": y_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "    \n",
    "    # Main workflow: Train ensemble\n",
    "    async def train_ensemble():\n",
    "        n_ensemble_models = 10\n",
    "        \n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(f\"Training {n_ensemble_models} surrogate models in parallel...\")\n",
    "        logger.info(\"=\"*70)\n",
    "        \n",
    "        # Generate diverse configurations\n",
    "        configs = generate(n_ensemble_models)\n",
    "        \n",
    "        # PARALLEL TRAINING: Submit all surrogate training tasks\n",
    "        training_tasks = []\n",
    "        for config in configs:\n",
    "            # Each call returns a future - they will run in parallel!\n",
    "            task = train(\n",
    "                model_id=config[\"model_id\"],\n",
    "                config=config,\n",
    "                train_data=train_data\n",
    "            )\n",
    "            training_tasks.append(task)\n",
    "        \n",
    "        logger.info(f\"Submitted {len(training_tasks)} parallel surrogate training tasks\")\n",
    "        \n",
    "        # Wait for all models to complete\n",
    "        surrogate_results = await asyncio.gather(*training_tasks)\n",
    "        \n",
    "        # Filter successful models\n",
    "        successful_results = [r for r in surrogate_results if r.get(\"success\", False)]\n",
    "        \n",
    "        logger.info(f\"Completed: {len(successful_results)}/{n_ensemble_models} models\")\n",
    "        \n",
    "        # Combine into ensemble\n",
    "        ensemble_result = await ensemble_predict(surrogate_results=successful_results)\n",
    "        \n",
    "        # Results\n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(\"ENSEMBLE TRAINING COMPLETE\")\n",
    "        logger.info(\"=\"*70)\n",
    "        logger.info(f\"Successfully trained: {len(successful_results)} models\")\n",
    "        \n",
    "        training_times = [r[\"training_time\"] for r in successful_results]\n",
    "        logger.info(f\"Average time per model: {np.mean(training_times):.2f}s\")\n",
    "        logger.info(f\"Total parallel time: {max(training_times):.2f}s\")\n",
    "        \n",
    "        logger.info(f\"Ensemble Statistics:\")\n",
    "        logger.info(f\"Best single model RMSE: {ensemble_result['best_model_rmse']:.4f}\")\n",
    "        logger.info(f\"Ensemble diversity: {ensemble_result['ensemble_diversity']:.4f}\")\n",
    "        logger.info(f\"Mean uncertainty: {np.mean(ensemble_result['total_uncertainty']):.4f}\")\n",
    "        \n",
    "        return ensemble_result\n",
    "\n",
    "    await train_ensemble()\n",
    "\n",
    "    # Cleanup\n",
    "    await learner.shutdown()\n",
    "    logging.getLogger().handlers.clear()\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5460bd8d",
   "metadata": {},
   "source": [
    "### ROSE+ SmartSim: In-Situ Data Aware Learner (work in progress)\n",
    "\n",
    "In traditional HPC workflows, simulations run first, write data to disk, and training happens later by reading those files.\n",
    "This is slow, expensive, and breaks interactivity.\n",
    "\n",
    "- Simulation and training run at the same time\n",
    "\n",
    "- Data is shared in memory (not dumped to disk)\n",
    "\n",
    "- Training can react immediately to simulation results\n",
    "\n",
    "This is critical when:\n",
    "\n",
    "- Simulations produce huge data\n",
    "\n",
    "- You want adaptive workflows (active learning, RL, steering)\n",
    "\n",
    "- You want to save time, storage, and energy\n",
    "\n",
    "Below we shoe the ROSE capabilites when integrated with SmartSim enabling in-memory, in-situ learning workflows.\n",
    "\n",
    "```\n",
    "┌──────────────┐     produces data     ┌──────────────────┐     consumes data     ┌──────────────┐\n",
    "│              │ ───────────────────▶  │                  │ ───────────────────▶  │              │\n",
    "│  Simulation  │                       │  In-Memory Store │                       │   Training   │\n",
    "│   (ROSE)     │ ◀───────────────────  │ (SmartSim/Redis) │ ◀───────────────────  │   (ROSE)     │\n",
    "│              │     new parameters    │                  │     model updates     │              │\n",
    "└──────────────┘                       └──────────────────┘                       └──────────────┘\n",
    "        ▲                                                                                 │\n",
    "        │                                                                                 │\n",
    "        └────────────────────────────── adaptive control loop ────────────────────────────┘\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cfaae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def in_situ_learner():\n",
    "\n",
    "    engine = await ConcurrentExecutionBackend(ProcessPoolExecutor())\n",
    "    asyncflow = await WorkflowEngine.create(engine)\n",
    "    learner = Learner(asyncflow)\n",
    "    init_default_logger(logging.INFO)\n",
    "    data_client = DataClient(\n",
    "        Client(address=REDIS_ADDRESS, cluster=False),\n",
    "        enable_tracking=True\n",
    "    )\n",
    "\n",
    "    REDIS_ADDRESS = \"127.0.0.1:6379\"\n",
    "\n",
    "    # -------------------------\n",
    "    # Simulation\n",
    "    # -------------------------\n",
    "    @learner.simulation_task(as_executable=False)\n",
    "    async def simulation(sim_id: int, params: np.ndarray):\n",
    "        client = DataClient(address=REDIS_ADDRESS, cluster=False) <- # Use Redis or use Dragon MP comm\n",
    "\n",
    "        result = params * 2.0 + np.random.rand(*params.shape)\n",
    "\n",
    "        client.put_tensor(f\"sim_output_{sim_id}\", result)\n",
    "        client.put_tensor(f\"sim_params_{sim_id}\", params)\n",
    "\n",
    "        logger.info(f\"[SIM] {sim_id} produced data\")\n",
    "        return sim_id\n",
    "\n",
    "    # -------------------------\n",
    "    # STREAMING training\n",
    "    # -------------------------\n",
    "    @learner.training_task(as_executable=False)\n",
    "    async def training_streaming(iteration: int, n_samples: int):\n",
    "        client = Client(address=REDIS_ADDRESS, cluster=False)\n",
    "\n",
    "        seen = 0\n",
    "        model_accumulator = []\n",
    "\n",
    "        while seen < n_samples:\n",
    "            key = f\"sim_output_{seen}\"\n",
    "\n",
    "            if client.tensor_exists(key):\n",
    "                output = client.get_tensor(key)\n",
    "                model_accumulator.append(output)\n",
    "\n",
    "                logger.info(f\"[TRAIN] consumed sim {seen}\")\n",
    "                seen += 1\n",
    "            else:\n",
    "                await asyncio.sleep(0.01)\n",
    "\n",
    "        model = np.mean(model_accumulator, axis=0)\n",
    "        client.put_tensor(f\"model_iter_{iteration}\", model)\n",
    "\n",
    "        logger.info(f\"[TRAIN] iteration {iteration} completed\")\n",
    "        return model\n",
    "\n",
    "    # -------------------------\n",
    "    # Main loop\n",
    "    # -------------------------\n",
    "    async def main():\n",
    "\n",
    "        n_iterations = 3\n",
    "        samples_per_iteration = 500\n",
    "\n",
    "        for iteration in range(n_iterations):\n",
    "            logger.info(f\"=== Iteration {iteration} ===\")\n",
    "\n",
    "            params_list = [np.random.rand(100) for _ in range(samples_per_iteration)]\n",
    "\n",
    "            # SIMULATIONS RUN CONCURRENTLY\n",
    "            sim_tasks = [(simulation(i, params))\n",
    "                for i, params in enumerate(params_list)\n",
    "            ]\n",
    "\n",
    "            # TRAINING\n",
    "            train_task = training_streaming(iteration, samples_per_iteration)\n",
    "\n",
    "            await asyncio.gather(train_task, *sim_tasks)\n",
    "\n",
    "        # Optional observability\n",
    "        metrics = await data_client.get_flow_metrics()\n",
    "        logger.info(f\"Total ops: {metrics.total_operations}\")\n",
    "        logger.info(f\"Read/write ratio: {metrics.read_write_ratio:.2f}\")\n",
    "\n",
    "        await data_client.export_stats(\"in_situ_stats.json\")\n",
    "\n",
    "    try:\n",
    "        await main()\n",
    "    finally:\n",
    "        await learner.shutdown()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(in_situ_learner())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
